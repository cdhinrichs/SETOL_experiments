{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46669e3-de34-46ba-b605-ef128792ffb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from weightwatcher import WeightWatcher\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "import plot_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dac3250a-600a-4b6a-a644-b0a8f6e1a8d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)of-00068.safetensors:  55%|███████████████████████████████████▍                             | 1.08G/1.98G [18:00:15<14:59:55, 16.7kB/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████| 14/14 [05:52<00:00, 25.16s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\"camelids/llama-13b-fp16-safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17f8193-0c0a-48af-9347-641f27303c60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:weightwatcher:unknown filter type <class 'numpy.ndarray'> detected and ignored\n"
     ]
    }
   ],
   "source": [
    "watcher = WeightWatcher(model=model) \n",
    "details = watcher.describe()\n",
    "layer_ids = details[details.layer_type=='LAYER_TYPE.DENSE'].layer_id.to_numpy()\n",
    "layer_ids = details.layer_id.to_numpy()\n",
    "\n",
    "layer_names = [str(x) for x in layer_ids]\n",
    "results =  watcher.analyze(layers=layer_ids, detX=True, svd_method=\"accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e56b94-830e-4d10-a652-3521ede9821b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results.to_pickle(\"llama-13b-details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c43eead-bf6b-43eb-98da-9d898096e161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"llama-13b-details\", \"rb\") as fp:\n",
    "  df13 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca38c20-e190-4ccd-91da-a230d115e4df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df13.plot.scatter(x='alpha', y='detX_delta')\n",
    "plt.title(\"Test of Trace Log Norm condition \\n\"+r\"LLAMA 13b: $\\Delta\\lambda_{min}$ (PL fit) - (|det X|=1)\", fontdict={'fontsize': plot_utils.MEDIUM_SIZE})\n",
    "plt.ylim((-2, 25))\n",
    "plt.xlabel(r\"PL $\\alpha$\")\n",
    "plt.ylabel(r\"$\\Delta\\lambda_{min}$\")\n",
    "plt.axvline(2, linewidth=0.5, color=\"red\", zorder=-1)\n",
    "plt.axhline(0, linewidth=0.5, color=\"red\", zorder=-1)\n",
    "plt.savefig(\"LLAMA_13b_ESD_trends.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62578029-8095-48b9-bf78-fefd1139c61b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    layer_id                             name         D     M      N        Q  \\\n",
      "0          1               model.embed_tokens  0.036168  8192  32000  3.90625   \n",
      "1          3     model.layers.0.mlp.down_proj  0.020997  8192  22016  2.68750   \n",
      "2          4     model.layers.0.mlp.gate_proj  0.020570  8192  22016  2.68750   \n",
      "3          5       model.layers.0.mlp.up_proj  0.028090  8192  22016  2.68750   \n",
      "4          7  model.layers.0.self_attn.k_proj  0.015414  8192   8192  1.00000   \n",
      "5          8  model.layers.0.self_attn.o_proj  0.017599  8192   8192  1.00000   \n",
      "6          9  model.layers.0.self_attn.q_proj  0.020537  8192   8192  1.00000   \n",
      "7         10  model.layers.0.self_attn.v_proj  0.035410  8192   8192  1.00000   \n",
      "8         12     model.layers.1.mlp.down_proj  0.008229  8192  22016  2.68750   \n",
      "9         13     model.layers.1.mlp.gate_proj  0.019634  8192  22016  2.68750   \n",
      "10        14       model.layers.1.mlp.up_proj  0.086498  8192  22016  2.68750   \n",
      "11        16  model.layers.1.self_attn.k_proj  0.033494  8192   8192  1.00000   \n",
      "12        17  model.layers.1.self_attn.o_proj  0.031361  8192   8192  1.00000   \n",
      "13        18  model.layers.1.self_attn.q_proj  0.039648  8192   8192  1.00000   \n",
      "14        19  model.layers.1.self_attn.v_proj  0.027287  8192   8192  1.00000   \n",
      "15        21     model.layers.2.mlp.down_proj  0.009107  8192  22016  2.68750   \n",
      "16        22     model.layers.2.mlp.gate_proj  0.016953  8192  22016  2.68750   \n",
      "17        23       model.layers.2.mlp.up_proj  0.068048  8192  22016  2.68750   \n",
      "18        25  model.layers.2.self_attn.k_proj  0.050959  8192   8192  1.00000   \n",
      "19        26  model.layers.2.self_attn.o_proj  0.014813  8192   8192  1.00000   \n",
      "20        27  model.layers.2.self_attn.q_proj  0.048317  8192   8192  1.00000   \n",
      "21        28  model.layers.2.self_attn.v_proj  0.042401  8192   8192  1.00000   \n",
      "22        30     model.layers.3.mlp.down_proj  0.022360  8192  22016  2.68750   \n",
      "23        31     model.layers.3.mlp.gate_proj  0.011307  8192  22016  2.68750   \n",
      "24        32       model.layers.3.mlp.up_proj  0.037097  8192  22016  2.68750   \n",
      "25        34  model.layers.3.self_attn.k_proj  0.035628  8192   8192  1.00000   \n",
      "26        35  model.layers.3.self_attn.o_proj  0.028946  8192   8192  1.00000   \n",
      "27        36  model.layers.3.self_attn.q_proj  0.028791  8192   8192  1.00000   \n",
      "28        37  model.layers.3.self_attn.v_proj  0.052211  8192   8192  1.00000   \n",
      "29        39     model.layers.4.mlp.down_proj  0.035540  8192  22016  2.68750   \n",
      "30        40     model.layers.4.mlp.gate_proj  0.010279  8192  22016  2.68750   \n",
      "31        41       model.layers.4.mlp.up_proj  0.053784  8192  22016  2.68750   \n",
      "32        43  model.layers.4.self_attn.k_proj  0.051191  8192   8192  1.00000   \n",
      "33        44  model.layers.4.self_attn.o_proj  0.037795  8192   8192  1.00000   \n",
      "34        45  model.layers.4.self_attn.q_proj  0.040450  8192   8192  1.00000   \n",
      "35        46  model.layers.4.self_attn.v_proj  0.041955  8192   8192  1.00000   \n",
      "36        47     model.layers.5.mlp.down_proj  0.035622  8192  22016  2.68750   \n",
      "37        48     model.layers.5.mlp.gate_proj  0.006349  8192  22016  2.68750   \n",
      "38        49  model.layers.5.self_attn.k_proj  0.030407  8192   8192  1.00000   \n",
      "39        50  model.layers.5.self_attn.o_proj  0.026974  8192   8192  1.00000   \n",
      "40        51  model.layers.5.self_attn.q_proj  0.038877  8192   8192  1.00000   \n",
      "41        52  model.layers.5.self_attn.v_proj  0.032395  8192   8192  1.00000   \n",
      "\n",
      "        alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0    1.903293        5.885552   -0.250705      2181  ...  0.013910   \n",
      "1    5.352391       13.801228    3.786516      3957  ...  0.261039   \n",
      "2    1.640219        3.761780   -0.354816      1543  ...  0.009364   \n",
      "3    1.638252        3.262224   -0.350135      1826  ...  0.009241   \n",
      "4    1.410180        4.166527   -0.317571       727  ...  0.006878   \n",
      "5    4.061762        8.023109    7.397690      3339  ...  0.309285   \n",
      "6    1.402161        4.112653   -0.322359       837  ...  0.006217   \n",
      "7    3.621269        4.913624    7.369447      2514  ...  0.169202   \n",
      "8    3.857422        8.643137    1.879913      5071  ...  0.109982   \n",
      "9    2.423818        6.249716    2.034807      2920  ...  0.050981   \n",
      "10   6.778529       11.786832    6.800027      3281  ...  0.529717   \n",
      "11   3.056230        9.001260   25.313574      1321  ...  0.257029   \n",
      "12   4.262903        7.550197    6.645070      3533  ...  0.374281   \n",
      "13   2.769192        8.321905   21.207195      1326  ...  0.195375   \n",
      "14   4.287643        6.277737    6.313948      3114  ...  0.227957   \n",
      "15   3.986411        7.764632    1.777214      5302  ...  0.113691   \n",
      "16   2.827654        7.135575    2.180157      3938  ...  0.070873   \n",
      "17  10.031059       20.444531    4.989806      4312  ...  0.946712   \n",
      "18   2.744152        7.944164   18.166711      1578  ...  0.183850   \n",
      "19   5.069472        8.898497    5.140822      3842  ...  0.339123   \n",
      "20   2.335897        6.649081   11.583999      1593  ...  0.112503   \n",
      "21   6.344940        8.029840    7.860169      3455  ...  0.563406   \n",
      "22   3.895244        7.122782    2.069463      5262  ...  0.126359   \n",
      "23   3.110656        7.485385    1.814788      4509  ...  0.075094   \n",
      "24  12.310151       19.826256    4.742132      4810  ...  1.811074   \n",
      "25   2.121819        5.378293    2.251093      2283  ...  0.045053   \n",
      "26   4.498026        7.940469    5.488351      3897  ...  0.292520   \n",
      "27   2.048792        5.112790    1.548497      2269  ...  0.037649   \n",
      "28   4.210557        6.509237    6.697871      3607  ...  0.278391   \n",
      "29   3.976914        7.442491    2.219596      5232  ...  0.136447   \n",
      "30   3.301440        7.892242    1.862657      4765  ...  0.085829   \n",
      "31  11.157486       17.882551    4.737772      5004  ...  1.886198   \n",
      "32   2.193263        5.577901    1.854360      2585  ...  0.041721   \n",
      "33   4.457440        7.400156    6.115699      3906  ...  0.328166   \n",
      "34   2.205999        5.473159    2.207930      2532  ...  0.046522   \n",
      "35   3.145764        4.622760    2.910771      3701  ...  0.090114   \n",
      "36   6.075980       11.085779    4.505706      5278  ...  0.602408   \n",
      "37   3.300638        8.213112    1.716624      4886  ...  0.082695   \n",
      "38   3.523401        9.102839   13.083908      2687  ...  0.275326   \n",
      "39   3.774493        6.814768    2.936231      4017  ...  0.123098   \n",
      "40   2.203195        5.472189    1.855370      2618  ...  0.042781   \n",
      "41   3.347060        4.810225    3.059744      3834  ...  0.104650   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max    sv_min        warning  \\\n",
      "0     1236.799438    36.819447  success  35.168159  0.425954   over-trained   \n",
      "1      378.892853    99.202652  success  19.465170  0.420273                  \n",
      "2      196.544937   149.897949  success  14.019448  0.062912   over-trained   \n",
      "3       98.012863   257.707092  success   9.900145  0.063023   over-trained   \n",
      "4      900.755249    20.239374  success  30.012585  0.000010   over-trained   \n",
      "5       94.466469    42.769909  success   9.719386  0.000001                  \n",
      "6      857.200928    21.910135  success  29.277994  0.000012   over-trained   \n",
      "7       22.744640   160.525391  success   4.769134  0.000030                  \n",
      "8      174.040909   243.083130  success  13.192456  0.614554                  \n",
      "9      378.843475   118.970917  success  19.463902  0.074110                  \n",
      "10      54.808529   686.799805  success   7.403278  0.068282  under-trained   \n",
      "11     881.489014    32.975319  success  29.689880  0.000005                  \n",
      "12      59.039078   102.866600  success   7.683689  0.000056                  \n",
      "13    1011.985413    29.357336  success  31.811718  0.000011                  \n",
      "14      29.116980   184.325943  success   5.396015  0.000035                  \n",
      "15      88.669647   487.363434  success   9.416456  0.681418                  \n",
      "16     333.807831   145.163437  success  18.270409  0.077712                  \n",
      "17     109.174911   371.715088  success  10.448680  0.069746  under-trained   \n",
      "18     785.131775    43.363274  success  28.020203  0.000039                  \n",
      "19      56.925968   155.390717  success   7.544930  0.000051                  \n",
      "20     702.228699    47.890320  success  26.499598  0.000040                  \n",
      "21      18.431055   444.514282  success   4.293140  0.000040  under-trained   \n",
      "22      67.388283   652.688599  success   8.209037  0.670173                  \n",
      "23     254.899200   192.704590  success  15.965563  0.081556                  \n",
      "24      40.790737  1035.645386  success   6.386763  0.078212  under-trained   \n",
      "25     342.575684    69.097557  success  18.508801  0.000060                  \n",
      "26      58.253639   181.197861  success   7.632407  0.000113                  \n",
      "27     312.978302    76.089561  success  17.691193  0.000004                  \n",
      "28      35.150593   281.122131  success   5.928793  0.000035                  \n",
      "29      74.374405   595.656372  success   8.624060  0.699888                  \n",
      "30     245.779327   199.951416  success  15.677351  0.091739                  \n",
      "31      40.062695  1074.728027  success   6.329510  0.080528  under-trained   \n",
      "32     349.299072    65.573090  success  18.689545  0.000003                  \n",
      "33      45.727852   252.800339  success   6.762237  0.000039                  \n",
      "34     302.715057    77.300201  success  17.398708  0.000088                  \n",
      "35      29.479441   373.762207  success   5.429497  0.000073                  \n",
      "36      66.761383   668.612854  success   8.170764  0.511262  under-trained   \n",
      "37     307.850983   162.636703  success  17.545683  0.103672                  \n",
      "38     383.298645    59.119358  success  19.578014  0.000061                  \n",
      "39      63.896809   186.003464  success   7.993548  0.000040                  \n",
      "40     304.615173    76.844772  success  17.453228  0.000118                  \n",
      "41      27.362114   421.716156  success   5.230881  0.000051                  \n",
      "\n",
      "    weak_rank_loss         xmax       xmin  \n",
      "0                0  1236.799438   1.212246  \n",
      "1                0   378.892853  19.509146  \n",
      "2                0   196.544937   0.286707  \n",
      "3                0    98.012863   0.293535  \n",
      "4               54   900.755249   0.013193  \n",
      "5               13    94.466469   3.875008  \n",
      "6               43   857.200928   0.012395  \n",
      "7               19    22.744640   3.485598  \n",
      "8                0   174.040909  12.159988  \n",
      "9                0   378.843475  13.678285  \n",
      "10               0    54.808529  33.325783  \n",
      "11              31   881.489014  91.187752  \n",
      "12               9    59.039078   5.265996  \n",
      "13              28  1011.985413  78.327789  \n",
      "14              13    29.116980   4.434250  \n",
      "15               0    88.669647  11.883154  \n",
      "16               0   333.807831  15.664552  \n",
      "17               0   109.174911  27.037271  \n",
      "18              24   785.131775  77.215256  \n",
      "19               7    56.925968   6.046870  \n",
      "20              19   702.228699  49.273617  \n",
      "21              10    18.431055   8.318096  \n",
      "22               0    67.388283  13.668105  \n",
      "23               0   254.899200  13.714141  \n",
      "24               0    40.790737  26.889006  \n",
      "25              12   342.575684   7.794434  \n",
      "26               7    58.253639   7.664561  \n",
      "27              11   312.978302   5.800639  \n",
      "28               9    35.150593   8.632739  \n",
      "29               0    74.374405  14.578479  \n",
      "30               0   245.779327  14.016113  \n",
      "31               0    40.062695  27.393162  \n",
      "32              11   349.299072   6.441764  \n",
      "33               7    45.727852   9.278995  \n",
      "34              10   302.715057   7.592607  \n",
      "35               7    29.479441   4.532166  \n",
      "36               0    66.761383  27.147329  \n",
      "37               0   307.850983  13.395030  \n",
      "38              11   383.298645  37.441887  \n",
      "39               7    63.896809   4.926704  \n",
      "40              10   304.615173   6.590473  \n",
      "41               8    27.362114   4.957418  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2     model.layers.10.mlp.down_proj  0.036479  8192  22016  2.6875   \n",
      "1          3     model.layers.10.mlp.gate_proj  0.011669  8192  22016  2.6875   \n",
      "2          4       model.layers.10.mlp.up_proj  0.050864  8192  22016  2.6875   \n",
      "3          6  model.layers.10.self_attn.k_proj  0.028896  8192   8192  1.0000   \n",
      "4          7  model.layers.10.self_attn.o_proj  0.049406  8192   8192  1.0000   \n",
      "5          8  model.layers.10.self_attn.q_proj  0.046169  8192   8192  1.0000   \n",
      "6          9  model.layers.10.self_attn.v_proj  0.058223  8192   8192  1.0000   \n",
      "7         10     model.layers.11.mlp.down_proj  0.040896  8192  22016  2.6875   \n",
      "8         11     model.layers.11.mlp.gate_proj  0.013264  8192  22016  2.6875   \n",
      "9         12  model.layers.11.self_attn.k_proj  0.028040  8192   8192  1.0000   \n",
      "10        13  model.layers.11.self_attn.o_proj  0.041559  8192   8192  1.0000   \n",
      "11        14  model.layers.11.self_attn.q_proj  0.039134  8192   8192  1.0000   \n",
      "12        15  model.layers.11.self_attn.v_proj  0.046110  8192   8192  1.0000   \n",
      "13        17        model.layers.5.mlp.up_proj  0.051737  8192  22016  2.6875   \n",
      "14        20      model.layers.6.mlp.down_proj  0.049756  8192  22016  2.6875   \n",
      "15        21      model.layers.6.mlp.gate_proj  0.007001  8192  22016  2.6875   \n",
      "16        22        model.layers.6.mlp.up_proj  0.055292  8192  22016  2.6875   \n",
      "17        24   model.layers.6.self_attn.k_proj  0.039113  8192   8192  1.0000   \n",
      "18        25   model.layers.6.self_attn.o_proj  0.032467  8192   8192  1.0000   \n",
      "19        26   model.layers.6.self_attn.q_proj  0.043863  8192   8192  1.0000   \n",
      "20        27   model.layers.6.self_attn.v_proj  0.037007  8192   8192  1.0000   \n",
      "21        29      model.layers.7.mlp.down_proj  0.045131  8192  22016  2.6875   \n",
      "22        30      model.layers.7.mlp.gate_proj  0.008402  8192  22016  2.6875   \n",
      "23        31        model.layers.7.mlp.up_proj  0.052031  8192  22016  2.6875   \n",
      "24        33   model.layers.7.self_attn.k_proj  0.034876  8192   8192  1.0000   \n",
      "25        34   model.layers.7.self_attn.o_proj  0.046118  8192   8192  1.0000   \n",
      "26        35   model.layers.7.self_attn.q_proj  0.035901  8192   8192  1.0000   \n",
      "27        36   model.layers.7.self_attn.v_proj  0.049686  8192   8192  1.0000   \n",
      "28        38      model.layers.8.mlp.down_proj  0.037395  8192  22016  2.6875   \n",
      "29        39      model.layers.8.mlp.gate_proj  0.006601  8192  22016  2.6875   \n",
      "30        40        model.layers.8.mlp.up_proj  0.050660  8192  22016  2.6875   \n",
      "31        42   model.layers.8.self_attn.k_proj  0.026609  8192   8192  1.0000   \n",
      "32        43   model.layers.8.self_attn.o_proj  0.039350  8192   8192  1.0000   \n",
      "33        44   model.layers.8.self_attn.q_proj  0.030156  8192   8192  1.0000   \n",
      "34        45   model.layers.8.self_attn.v_proj  0.042770  8192   8192  1.0000   \n",
      "35        47      model.layers.9.mlp.down_proj  0.040477  8192  22016  2.6875   \n",
      "36        48      model.layers.9.mlp.gate_proj  0.007081  8192  22016  2.6875   \n",
      "37        49        model.layers.9.mlp.up_proj  0.048195  8192  22016  2.6875   \n",
      "38        51   model.layers.9.self_attn.k_proj  0.042892  8192   8192  1.0000   \n",
      "39        52   model.layers.9.self_attn.o_proj  0.047330  8192   8192  1.0000   \n",
      "40        53   model.layers.9.self_attn.q_proj  0.035815  8192   8192  1.0000   \n",
      "41        54   model.layers.9.self_attn.v_proj  0.038464  8192   8192  1.0000   \n",
      "\n",
      "       alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   3.935046        7.417185    2.453180      5203  ...  0.146387   \n",
      "1   3.298229        8.680705    1.791808      5136  ...  0.089868   \n",
      "2   4.854906        8.095748    2.443726      5382  ...  0.204886   \n",
      "3   2.772792        7.360385    4.871812      2965  ...  0.102352   \n",
      "4   3.710218        6.167910    3.666178      3844  ...  0.138486   \n",
      "5   2.764539        7.001992    5.872522      2892  ...  0.111823   \n",
      "6   3.527051        4.978465    3.535232      3824  ...  0.123455   \n",
      "7   3.621541        6.758688    1.944938      5172  ...  0.103545   \n",
      "8   3.339771        8.941827    1.744603      5140  ...  0.088625   \n",
      "9   2.632445        7.087074    3.899883      2931  ...  0.081930   \n",
      "10  4.849748        8.044017    5.274946      3990  ...  0.328906   \n",
      "11  2.687456        6.995137    4.448376      2929  ...  0.090587   \n",
      "12  4.148724        5.875893    4.309440      3942  ...  0.196412   \n",
      "13  9.953123       15.948304    4.588471      5146  ...  1.513354   \n",
      "14  3.573067        6.509815    1.615051      5214  ...  0.086689   \n",
      "15  3.439005        8.783538    1.629276      5063  ...  0.085487   \n",
      "16  8.584867       13.706129    3.902031      5312  ...  0.987466   \n",
      "17  3.044903        8.070784    9.374305      2766  ...  0.180746   \n",
      "18  3.923950        6.827762    3.850980      3987  ...  0.169380   \n",
      "19  2.735838        6.947415    6.608255      2732  ...  0.116765   \n",
      "20  3.706368        5.274493    3.698695      3876  ...  0.143036   \n",
      "21  3.394334        6.106112    1.586572      5140  ...  0.079855   \n",
      "22  3.381804        8.930560    1.424632      5136  ...  0.076162   \n",
      "23  4.721535        7.393273    1.910304      5404  ...  0.149220   \n",
      "24  2.509650        6.670647    3.429122      2738  ...  0.069341   \n",
      "25  5.561589        9.650161    7.503195      3965  ...  0.626582   \n",
      "26  2.391757        6.148841    2.884137      2724  ...  0.059562   \n",
      "27  3.808683        5.282494    3.315497      3922  ...  0.133294   \n",
      "28  3.230873        5.982027    1.569531      5065  ...  0.074612   \n",
      "29  3.388359        8.830520    1.494492      5183  ...  0.079879   \n",
      "30  5.101653        8.223205    2.203826      5429  ...  0.198030   \n",
      "31  2.683951        7.300104    5.089559      2706  ...  0.096265   \n",
      "32  3.704789        6.316276    4.059919      3885  ...  0.158286   \n",
      "33  2.567947        6.687449    3.846734      2745  ...  0.076057   \n",
      "34  3.389064        5.104475    3.089067      3857  ...  0.107164   \n",
      "35  5.152451       10.021778    6.017422      5094  ...  0.581459   \n",
      "36  3.342343        8.552701    1.520505      5184  ...  0.079827   \n",
      "37  4.604246        7.543013    2.005863      5410  ...  0.153268   \n",
      "38  2.504103        6.601743    3.612944      2762  ...  0.070747   \n",
      "39  3.995758        6.679886    4.411682      3873  ...  0.188715   \n",
      "40  4.173905       10.645069   21.520816      2750  ...  0.622454   \n",
      "41  3.292400        4.766341    3.197419      3822  ...  0.105853   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max    sv_min        warning  \\\n",
      "0       76.719261   599.343384  success   8.758953  0.487980                  \n",
      "1      428.478516   118.198875  success  20.699722  0.234443                  \n",
      "2       46.509262   977.925171  success   6.819770  0.250375                  \n",
      "3      451.338806    50.564564  success  21.244736  0.000047                  \n",
      "4       45.963348   275.852875  success   6.779627  0.000145                  \n",
      "5      341.026459    65.569939  success  18.466902  0.000036                  \n",
      "6       25.793442   500.417450  success   5.078724  0.000025                  \n",
      "7       73.493126   633.202637  success   8.572813  0.615370                  \n",
      "8      475.748535   105.459122  success  21.811661  0.275119                  \n",
      "9      492.268982    44.238827  success  22.187136  0.000011                  \n",
      "10      45.566574   288.060242  success   6.750302  0.000061                  \n",
      "11     400.759705    52.930244  success  20.018984  0.000042                  \n",
      "12      26.080341   507.308655  success   5.106892  0.000017                  \n",
      "13      40.025948  1085.407837  success   6.326606  0.094839  under-trained   \n",
      "14      66.360916   677.968262  success   8.146221  0.420024                  \n",
      "15     358.172852   141.590652  success  18.925455  0.122148                  \n",
      "16      39.495296  1115.904785  success   6.284528  0.107856  under-trained   \n",
      "17     447.288757    48.792015  success  21.149202  0.000031                  \n",
      "18      54.956947   224.312912  success   7.413295  0.000116                  \n",
      "19     346.266388    62.776119  success  18.608234  0.000061                  \n",
      "20      26.490463   452.661987  success   5.146889  0.000068                  \n",
      "21      62.938080   713.603516  success   7.933352  0.410234                  \n",
      "22     437.288971   117.983803  success  20.911455  0.135430                  \n",
      "23      36.801197  1199.820068  success   6.066399  0.130157                  \n",
      "24     454.987518    45.490200  success  21.330437  0.000026                  \n",
      "25      54.343105   232.454132  success   7.371778  0.000054                  \n",
      "26     372.260437    54.747650  success  19.294052  0.000008                  \n",
      "27      24.375898   514.640564  success   4.937195  0.000203                  \n",
      "28      71.042801   637.812378  success   8.428689  0.592528                  \n",
      "29     403.770752   126.507034  success  20.094048  0.183613                  \n",
      "30      40.913872  1094.435425  success   6.396395  0.163114                  \n",
      "31     524.698181    39.609734  success  22.906292  0.000040                  \n",
      "32      50.686794   238.719299  success   7.119466  0.000147                  \n",
      "33     401.976318    49.750935  success  20.049347  0.000007                  \n",
      "34      32.074577   376.499054  success   5.663442  0.000046                  \n",
      "35      88.115181   520.042786  success   9.386969  0.693877                  \n",
      "36     362.154053   139.595398  success  19.030346  0.211184                  \n",
      "37      43.478352  1041.522339  success   6.593812  0.205690                  \n",
      "38     432.883026    48.286816  success  20.805841  0.000022                  \n",
      "39      46.961773   249.187424  success   6.852866  0.000009                  \n",
      "40     355.128601    57.470203  success  18.844856  0.000022                  \n",
      "41      28.033663   423.362000  success   5.294683  0.000050                  \n",
      "\n",
      "    weak_rank_loss        xmax       xmin  \n",
      "0                0   76.719261  16.445681  \n",
      "1                0  428.478516  14.024226  \n",
      "2                0   46.509262  16.210546  \n",
      "3               13  451.338806  14.837500  \n",
      "4                6   45.963348   6.386508  \n",
      "5               14  341.026459  17.272619  \n",
      "6                7   25.793442   6.296363  \n",
      "7                0   73.493126  13.755967  \n",
      "8                0  475.748535  13.604028  \n",
      "9               22  492.268982  11.572842  \n",
      "10               6   45.566574   9.188687  \n",
      "11              21  400.759705  12.695361  \n",
      "12               6   26.080341   7.703385  \n",
      "13               0   40.025948  26.851801  \n",
      "14               0   66.360916  11.488425  \n",
      "15               0  358.172852  13.031855  \n",
      "16               0   39.495296  23.549833  \n",
      "17              13  447.288757  26.175140  \n",
      "18               6   54.956947   6.487674  \n",
      "19              13  346.266388  18.736599  \n",
      "20               8   26.490463   6.087006  \n",
      "21               0   62.938080  11.311326  \n",
      "22               0  437.288971  11.971770  \n",
      "23               0   36.801197  12.861332  \n",
      "24              14  454.987518   9.806245  \n",
      "25               5   54.343105  12.280381  \n",
      "26              15  372.260437   8.300748  \n",
      "27               6   24.375898   5.780760  \n",
      "28               0   71.042801  11.320011  \n",
      "29               0  403.770752  12.288517  \n",
      "30               0   40.913872  14.647311  \n",
      "31              22  524.698181  14.057495  \n",
      "32               7   50.686794   6.675726  \n",
      "33              20  401.976318  10.492135  \n",
      "34               7   32.074577   5.232168  \n",
      "35               0   88.115181  36.326897  \n",
      "36               0  362.154053  12.325621  \n",
      "37               0   43.478352  13.716840  \n",
      "38              19  432.883026  10.375356  \n",
      "39               7   46.961773   6.958338  \n",
      "40              17  355.128601  54.743279  \n",
      "41               7   28.033663   5.298162  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.11.mlp.up_proj  0.046334  8192  22016  2.6875   \n",
      "1          5     model.layers.12.mlp.down_proj  0.047582  8192  22016  2.6875   \n",
      "2          6     model.layers.12.mlp.gate_proj  0.015746  8192  22016  2.6875   \n",
      "3          7       model.layers.12.mlp.up_proj  0.045333  8192  22016  2.6875   \n",
      "4          9  model.layers.12.self_attn.k_proj  0.031146  8192   8192  1.0000   \n",
      "5         10  model.layers.12.self_attn.o_proj  0.034590  8192   8192  1.0000   \n",
      "6         11  model.layers.12.self_attn.q_proj  0.035097  8192   8192  1.0000   \n",
      "7         12  model.layers.12.self_attn.v_proj  0.051990  8192   8192  1.0000   \n",
      "8         14     model.layers.13.mlp.down_proj  0.046318  8192  22016  2.6875   \n",
      "9         15     model.layers.13.mlp.gate_proj  0.016213  8192  22016  2.6875   \n",
      "10        16       model.layers.13.mlp.up_proj  0.049672  8192  22016  2.6875   \n",
      "11        18  model.layers.13.self_attn.k_proj  0.016315  8192   8192  1.0000   \n",
      "12        19  model.layers.13.self_attn.o_proj  0.045461  8192   8192  1.0000   \n",
      "13        20  model.layers.13.self_attn.q_proj  0.027966  8192   8192  1.0000   \n",
      "14        21  model.layers.13.self_attn.v_proj  0.053149  8192   8192  1.0000   \n",
      "15        23     model.layers.14.mlp.down_proj  0.041623  8192  22016  2.6875   \n",
      "16        24     model.layers.14.mlp.gate_proj  0.018213  8192  22016  2.6875   \n",
      "17        25       model.layers.14.mlp.up_proj  0.042868  8192  22016  2.6875   \n",
      "18        27  model.layers.14.self_attn.k_proj  0.015828  8192   8192  1.0000   \n",
      "19        28  model.layers.14.self_attn.o_proj  0.030464  8192   8192  1.0000   \n",
      "20        29  model.layers.14.self_attn.q_proj  0.035852  8192   8192  1.0000   \n",
      "21        30  model.layers.14.self_attn.v_proj  0.038910  8192   8192  1.0000   \n",
      "22        32     model.layers.15.mlp.down_proj  0.042577  8192  22016  2.6875   \n",
      "23        33     model.layers.15.mlp.gate_proj  0.012712  8192  22016  2.6875   \n",
      "24        34       model.layers.15.mlp.up_proj  0.036998  8192  22016  2.6875   \n",
      "25        36  model.layers.15.self_attn.k_proj  0.012092  8192   8192  1.0000   \n",
      "26        37  model.layers.15.self_attn.o_proj  0.041709  8192   8192  1.0000   \n",
      "27        38  model.layers.15.self_attn.q_proj  0.026318  8192   8192  1.0000   \n",
      "28        39  model.layers.15.self_attn.v_proj  0.047060  8192   8192  1.0000   \n",
      "29        41     model.layers.16.mlp.down_proj  0.037619  8192  22016  2.6875   \n",
      "30        42     model.layers.16.mlp.gate_proj  0.012832  8192  22016  2.6875   \n",
      "31        43       model.layers.16.mlp.up_proj  0.039792  8192  22016  2.6875   \n",
      "32        45  model.layers.16.self_attn.k_proj  0.014416  8192   8192  1.0000   \n",
      "33        46  model.layers.16.self_attn.o_proj  0.043757  8192   8192  1.0000   \n",
      "34        47  model.layers.16.self_attn.q_proj  0.023187  8192   8192  1.0000   \n",
      "35        48  model.layers.16.self_attn.v_proj  0.050559  8192   8192  1.0000   \n",
      "36        49     model.layers.17.mlp.down_proj  0.034047  8192  22016  2.6875   \n",
      "37        50     model.layers.17.mlp.gate_proj  0.019012  8192  22016  2.6875   \n",
      "38        51  model.layers.17.self_attn.k_proj  0.016018  8192   8192  1.0000   \n",
      "39        52  model.layers.17.self_attn.o_proj  0.042386  8192   8192  1.0000   \n",
      "40        53  model.layers.17.self_attn.q_proj  0.033379  8192   8192  1.0000   \n",
      "41        54  model.layers.17.self_attn.v_proj  0.046201  8192   8192  1.0000   \n",
      "\n",
      "       alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   5.761966        9.658919    3.263060      5381  ...  0.391431   \n",
      "1   3.426218        6.472874    1.550706      5170  ...  0.079388   \n",
      "2   3.410150        9.172435    2.412610      5105  ...  0.121422   \n",
      "3   4.989889        8.468793    2.800222      5327  ...  0.247920   \n",
      "4   2.667848        7.279948    4.393590      2944  ...  0.090853   \n",
      "5   6.391487       10.734886    8.722946      3930  ...  0.984346   \n",
      "6   2.715501        7.121390    5.393361      2917  ...  0.105382   \n",
      "7   3.718494        5.388508    3.901834      3887  ...  0.148086   \n",
      "8   3.270222        6.191717    1.476260      5114  ...  0.072080   \n",
      "9   3.454024        9.322617    2.267962      5111  ...  0.116201   \n",
      "10  4.527764        7.725617    2.237009      5327  ...  0.163772   \n",
      "11  2.587276        6.908166    3.413182      2972  ...  0.075329   \n",
      "12  6.653024       11.057182    9.404172      3946  ...  1.205229   \n",
      "13  2.623930        6.835257    4.463813      2943  ...  0.089804   \n",
      "14  3.380139        4.680655    2.653481      3959  ...  0.094379   \n",
      "15  3.243921        6.216815    1.633996      5072  ...  0.077239   \n",
      "16  3.446762        9.276571    1.860120      5152  ...  0.096944   \n",
      "17  5.019951        8.736219    2.751782      5358  ...  0.247881   \n",
      "18  2.832488        7.522109    4.686157      3006  ...  0.104247   \n",
      "19  4.516240        9.830329    5.996385      3819  ...  0.312016   \n",
      "20  2.681581        6.969945    4.171586      2980  ...  0.087898   \n",
      "21  5.247525       10.285350    7.977245      3891  ...  0.589026   \n",
      "22  3.260050        6.357788    1.588639      5091  ...  0.076186   \n",
      "23  3.491578        9.389885    2.237474      5135  ...  0.117194   \n",
      "24  4.458160        7.875049    2.343480      5313  ...  0.170371   \n",
      "25  2.744924        7.445650    4.146338      3054  ...  0.093672   \n",
      "26  3.389745        5.326083    2.802682      3920  ...  0.100095   \n",
      "27  2.715310        7.142320    4.717498      3010  ...  0.098705   \n",
      "28  3.589244        5.147964    3.134435      3958  ...  0.118553   \n",
      "29  3.238018        6.253887    1.446766      5151  ...  0.071201   \n",
      "30  3.523130        9.533436    2.225079      5132  ...  0.117770   \n",
      "31  5.178312        9.102344    3.253590      5305  ...  0.316757   \n",
      "32  2.669160        7.110498    3.848699      2996  ...  0.084848   \n",
      "33  6.376635       11.232430    9.723906      3928  ...  1.075327   \n",
      "34  2.747291        7.149408    4.497892      2985  ...  0.096040   \n",
      "35  3.989355        5.619862    3.904389      3947  ...  0.167372   \n",
      "36  3.326267        6.528982    1.573405      5203  ...  0.079325   \n",
      "37  3.487326        9.347620    2.051681      5150  ...  0.107637   \n",
      "38  2.824466        7.535954    5.337259      3014  ...  0.115854   \n",
      "39  3.944076        6.620896    3.956939      3957  ...  0.169694   \n",
      "40  2.881265        7.554034    6.132287      2979  ...  0.126835   \n",
      "41  4.988966        7.017905    5.256412      4024  ...  0.342051   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max        sv_min  \\\n",
      "0       47.459511   972.336121  success   6.889086  2.968075e-01   \n",
      "1       77.485146   604.990967  success   8.802565  4.228790e-01   \n",
      "2      489.491913   101.354652  success  22.124464  2.799825e-01   \n",
      "3       49.795559   931.344849  success   7.056597  3.201227e-01   \n",
      "4      535.514648    40.174717  success  23.141190  3.052629e-05   \n",
      "5       47.814533   292.791229  success   6.914805  1.693728e-05   \n",
      "6      419.271637    50.849155  success  20.476124  1.108844e-05   \n",
      "7       28.126154   499.747681  success   5.303410  6.429806e-05   \n",
      "8       78.228127   601.122009  success   8.844666  3.861278e-01   \n",
      "9      500.103973    98.941589  success  22.363005  2.622260e-01   \n",
      "10      50.848320   917.675354  success   7.130801  3.050538e-01   \n",
      "11     467.793335    44.224895  success  21.628531  4.128965e-05   \n",
      "12      45.917519   297.071960  success   6.776247  1.747707e-05   \n",
      "13     402.688843    50.592384  success  20.067108  1.619093e-05   \n",
      "14      24.252254   557.351746  success   4.924658  1.797183e-05   \n",
      "15      82.499405   574.424377  success   9.082918  6.393120e-01   \n",
      "16     491.345428    99.259216  success  22.166313  3.313411e-01   \n",
      "17      54.992004   859.016052  success   7.415659  3.312058e-01   \n",
      "18     452.537231    43.171165  success  21.272923  6.786206e-07   \n",
      "19     150.197281    91.634201  success  12.255500  7.313491e-05   \n",
      "20     397.367462    48.797932  success  19.934078  5.226981e-05   \n",
      "21      91.209114   150.419556  success   9.550346  6.866408e-05   \n",
      "22      89.168587   531.803894  success   9.442912  4.908929e-01   \n",
      "23     488.984711    98.839905  success  22.112999  2.895058e-01   \n",
      "24      58.402943   808.662842  success   7.642182  3.121201e-01   \n",
      "25     515.841553    40.425369  success  22.712145  2.342003e-05   \n",
      "26      37.259247   382.393219  success   6.104035  2.510251e-05   \n",
      "27     426.960846    48.198956  success  20.663031  1.646774e-05   \n",
      "28      27.181627   522.383240  success   5.213600  5.145152e-05   \n",
      "29      85.387375   557.216003  success   9.240529  4.407858e-01   \n",
      "30     508.108551    94.376259  success  22.541264  2.908650e-01   \n",
      "31      57.250870   825.914429  success   7.566431  3.252902e-01   \n",
      "32     461.260590    42.763912  success  21.476978  2.592388e-05   \n",
      "33      57.742813   246.090668  success   7.598869  5.402330e-05   \n",
      "34     400.265167    48.699986  success  20.006628  5.016576e-05   \n",
      "35      25.627974   552.942078  success   5.062408  8.806359e-05   \n",
      "36      91.802788   517.947754  success   9.581377  4.501345e-01   \n",
      "37     479.131409    99.633255  success  21.889071  3.073334e-01   \n",
      "38     465.692047    38.920750  success  21.579899  1.856611e-05   \n",
      "39      47.719299   302.935181  success   6.907916  3.982788e-05   \n",
      "40     418.578461    43.629299  success  20.459190  1.945284e-05   \n",
      "41      25.508509   571.127197  success   5.050595  1.074826e-04   \n",
      "\n",
      "          warning  weak_rank_loss        xmax       xmin  \n",
      "0                               0   47.459511  21.063492  \n",
      "1                               0   77.485146  11.602029  \n",
      "2                               0  489.491913  17.498970  \n",
      "3                               0   49.795559  18.548044  \n",
      "4                              19  535.514648  12.732972  \n",
      "5   under-trained               5   47.814533  15.693316  \n",
      "6                              17  419.271637  15.220718  \n",
      "7                               5   28.126154   7.487259  \n",
      "8                               0   78.228127  11.211755  \n",
      "9                               0  500.103973  16.578039  \n",
      "10                              0   50.848320  15.454935  \n",
      "11                             22  467.793335   9.768738  \n",
      "12  under-trained               6   45.917519  16.425928  \n",
      "13                             20  402.688843  12.235132  \n",
      "14                              5   24.252254   5.138508  \n",
      "15                              0   82.499405  12.210267  \n",
      "16                              0  491.345428  13.913346  \n",
      "17                              0   54.992004  18.614719  \n",
      "18                             20  452.537231  12.259070  \n",
      "19                              8  150.197281  10.846467  \n",
      "20                             20  397.367462  10.952280  \n",
      "21                              6   91.209114  14.130623  \n",
      "22                              0   89.168587  11.956647  \n",
      "23                              0  488.984711  16.013584  \n",
      "24                              0   58.402943  16.257978  \n",
      "25                             23  515.841553  11.715165  \n",
      "26                              7   37.259247   5.676580  \n",
      "27                             21  426.960846  12.995751  \n",
      "28                              5   27.181627   6.232131  \n",
      "29                              0   85.387375  11.172856  \n",
      "30                              0  508.108551  15.814776  \n",
      "31                              0   57.250870  21.528975  \n",
      "32                             21  461.260590  10.359517  \n",
      "33  under-trained               5   57.742813  17.666204  \n",
      "34                             20  400.265167  11.785016  \n",
      "35                              5   25.627974   7.551946  \n",
      "36                              0   91.802788  11.900423  \n",
      "37                              0  479.131409  14.733511  \n",
      "38                             29  465.692047  12.812545  \n",
      "39                              5   47.719299   7.794705  \n",
      "40                             24  418.578461  14.681585  \n",
      "41                              5   25.508509  10.167505  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.17.mlp.up_proj  0.041322  8192  22016  2.6875   \n",
      "1          5     model.layers.18.mlp.down_proj  0.028358  8192  22016  2.6875   \n",
      "2          6     model.layers.18.mlp.gate_proj  0.017386  8192  22016  2.6875   \n",
      "3          7       model.layers.18.mlp.up_proj  0.042499  8192  22016  2.6875   \n",
      "4          9  model.layers.18.self_attn.k_proj  0.016482  8192   8192  1.0000   \n",
      "5         10  model.layers.18.self_attn.o_proj  0.042063  8192   8192  1.0000   \n",
      "6         11  model.layers.18.self_attn.q_proj  0.025085  8192   8192  1.0000   \n",
      "7         12  model.layers.18.self_attn.v_proj  0.058652  8192   8192  1.0000   \n",
      "8         14     model.layers.19.mlp.down_proj  0.021845  8192  22016  2.6875   \n",
      "9         15     model.layers.19.mlp.gate_proj  0.016923  8192  22016  2.6875   \n",
      "10        16       model.layers.19.mlp.up_proj  0.043164  8192  22016  2.6875   \n",
      "11        18  model.layers.19.self_attn.k_proj  0.016875  8192   8192  1.0000   \n",
      "12        19  model.layers.19.self_attn.o_proj  0.043074  8192   8192  1.0000   \n",
      "13        20  model.layers.19.self_attn.q_proj  0.027163  8192   8192  1.0000   \n",
      "14        21  model.layers.19.self_attn.v_proj  0.043364  8192   8192  1.0000   \n",
      "15        23     model.layers.20.mlp.down_proj  0.021120  8192  22016  2.6875   \n",
      "16        24     model.layers.20.mlp.gate_proj  0.015166  8192  22016  2.6875   \n",
      "17        25       model.layers.20.mlp.up_proj  0.039059  8192  22016  2.6875   \n",
      "18        27  model.layers.20.self_attn.k_proj  0.022283  8192   8192  1.0000   \n",
      "19        28  model.layers.20.self_attn.o_proj  0.024784  8192   8192  1.0000   \n",
      "20        29  model.layers.20.self_attn.q_proj  0.025155  8192   8192  1.0000   \n",
      "21        30  model.layers.20.self_attn.v_proj  0.029980  8192   8192  1.0000   \n",
      "22        32     model.layers.21.mlp.down_proj  0.021776  8192  22016  2.6875   \n",
      "23        33     model.layers.21.mlp.gate_proj  0.014053  8192  22016  2.6875   \n",
      "24        34       model.layers.21.mlp.up_proj  0.036604  8192  22016  2.6875   \n",
      "25        36  model.layers.21.self_attn.k_proj  0.022799  8192   8192  1.0000   \n",
      "26        37  model.layers.21.self_attn.o_proj  0.023983  8192   8192  1.0000   \n",
      "27        38  model.layers.21.self_attn.q_proj  0.023544  8192   8192  1.0000   \n",
      "28        39  model.layers.21.self_attn.v_proj  0.022944  8192   8192  1.0000   \n",
      "29        41     model.layers.22.mlp.down_proj  0.018907  8192  22016  2.6875   \n",
      "30        42     model.layers.22.mlp.gate_proj  0.015015  8192  22016  2.6875   \n",
      "31        43       model.layers.22.mlp.up_proj  0.039753  8192  22016  2.6875   \n",
      "32        45  model.layers.22.self_attn.k_proj  0.021243  8192   8192  1.0000   \n",
      "33        46  model.layers.22.self_attn.o_proj  0.032919  8192   8192  1.0000   \n",
      "34        47  model.layers.22.self_attn.q_proj  0.025615  8192   8192  1.0000   \n",
      "35        48  model.layers.22.self_attn.v_proj  0.029898  8192   8192  1.0000   \n",
      "36        49     model.layers.23.mlp.down_proj  0.024262  8192  22016  2.6875   \n",
      "37        50     model.layers.23.mlp.gate_proj  0.021051  8192  22016  2.6875   \n",
      "38        51  model.layers.23.self_attn.k_proj  0.022123  8192   8192  1.0000   \n",
      "39        52  model.layers.23.self_attn.o_proj  0.017974  8192   8192  1.0000   \n",
      "40        53  model.layers.23.self_attn.q_proj  0.023371  8192   8192  1.0000   \n",
      "41        54  model.layers.23.self_attn.v_proj  0.018650  8192   8192  1.0000   \n",
      "\n",
      "       alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   4.747001        8.443049    2.752449      5329  ...  0.225953   \n",
      "1   3.366368        6.494315    1.519736      5285  ...  0.079680   \n",
      "2   3.619720        9.703073    2.347430      5193  ...  0.130498   \n",
      "3   5.120279        9.170725    3.145717      5366  ...  0.306258   \n",
      "4   2.733145        7.266654    4.352052      3019  ...  0.095551   \n",
      "5   6.379192       11.073063    8.584353      3950  ...  0.936397   \n",
      "6   2.670903        6.943018    4.031142      3003  ...  0.086516   \n",
      "7   3.677351        5.017847    2.738894      4040  ...  0.109030   \n",
      "8   3.491327        6.790218    1.649661      5368  ...  0.091092   \n",
      "9   3.607236        9.623796    1.966367      5231  ...  0.110078   \n",
      "10  5.265554        9.471412    3.127237      5412  ...  0.324304   \n",
      "11  2.809369        7.478273    5.371906      2935  ...  0.114206   \n",
      "12  4.890470        7.412193    4.317465      4112  ...  0.271722   \n",
      "13  2.919044        7.692377    6.881168      2932  ...  0.143436   \n",
      "14  7.385863        9.759937    5.643934      4151  ...  0.768768   \n",
      "15  3.612381        7.119063    1.589229      5435  ...  0.092827   \n",
      "16  3.607979        9.562252    1.860016      5250  ...  0.104824   \n",
      "17  4.422482        8.046831    2.053664      5430  ...  0.150813   \n",
      "18  2.979485        7.992945    5.052469      2995  ...  0.117877   \n",
      "19  5.426715        9.512233    6.399169      4011  ...  0.536818   \n",
      "20  2.859155        7.638967    4.360518      2992  ...  0.099093   \n",
      "21  6.120917        9.934557    7.173090      4051  ...  0.919744   \n",
      "22  3.899713        7.288073    1.718317      5525  ...  0.111776   \n",
      "23  3.703308        9.758631    2.038338      5269  ...  0.119238   \n",
      "24  4.616127        8.349520    2.234209      5442  ...  0.177723   \n",
      "25  2.711548        7.442317    4.455884      2759  ...  0.092013   \n",
      "26  5.909022       10.015209    6.200174      4242  ...  0.776184   \n",
      "27  2.889777        7.874453    6.790571      2778  ...  0.137099   \n",
      "28  5.434835        7.373325    3.850652      4207  ...  0.295001   \n",
      "29  4.065889        7.448195    1.775897      5575  ...  0.123328   \n",
      "30  3.855447       10.098652    2.351357      5285  ...  0.146289   \n",
      "31  4.792762        8.576525    2.161277      5466  ...  0.179592   \n",
      "32  2.870685        7.524815    4.385961      2895  ...  0.099007   \n",
      "33  7.056464       11.616696    5.905203      4256  ...  0.945861   \n",
      "34  2.890058        7.673965    4.747972      2908  ...  0.106324   \n",
      "35  8.618450       11.208922    5.413379      4207  ...  1.027272   \n",
      "36  4.332658        7.762731    1.976113      5609  ...  0.152432   \n",
      "37  4.154242       10.806134    2.935178      5299  ...  0.208438   \n",
      "38  3.249712        8.808070   10.477105      2838  ...  0.244016   \n",
      "39  5.570984        9.284958    3.990558      4223  ...  0.324028   \n",
      "40  3.205934        8.662926   10.710119      2879  ...  0.251389   \n",
      "41  5.706244        8.441881    4.015958      4181  ...  0.331953   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max    sv_min        warning  \\\n",
      "0       60.062981   787.252014  success   7.750031  0.349420                  \n",
      "1       84.952370   560.109375  success   9.216961  0.446088                  \n",
      "2      479.307434    98.992424  success  21.893091  0.310276                  \n",
      "3       61.810150   765.848145  success   7.861943  0.355369                  \n",
      "4      455.737671    41.912163  success  21.348013  0.000020                  \n",
      "5       54.426411   261.547302  success   7.377426  0.000154  under-trained   \n",
      "6      397.651245    47.274662  success  19.941195  0.000020                  \n",
      "7       23.148758   614.710022  success   4.811316  0.000045                  \n",
      "8       88.080826   539.136841  success   9.385139  0.450115                  \n",
      "9      465.493744   101.529999  success  21.575304  0.320438                  \n",
      "10      62.914303   750.925476  success   7.931854  0.341939                  \n",
      "11     459.097290    36.091221  success  21.426556  0.000026                  \n",
      "12      32.782341   446.706573  success   5.725586  0.000129                  \n",
      "13     431.755951    38.391582  success  20.778738  0.000015                  \n",
      "14      20.962111   703.779358  success   4.578440  0.000109  under-trained   \n",
      "15      93.484482   505.842010  success   9.668737  0.643826                  \n",
      "16     446.999084   105.780540  success  21.142353  0.310367                  \n",
      "17      65.997604   712.020996  success   8.123891  0.346325                  \n",
      "18     481.570282    34.752274  success  21.944710  0.000038                  \n",
      "19      56.604767   259.649078  success   7.523614  0.000021                  \n",
      "20     469.630981    35.657494  success  21.670971  0.000004                  \n",
      "21      41.980766   347.962860  success   6.479257  0.000050  under-trained   \n",
      "22      73.939095   638.216614  success   8.598784  0.567774                  \n",
      "23     431.630402   109.431862  success  20.775717  0.332457                  \n",
      "24      64.383034   725.994385  success   8.023904  0.344969                  \n",
      "25     555.487854    27.641157  success  23.568790  0.000006                  \n",
      "26      49.533775   302.175232  success   7.038023  0.000017                  \n",
      "27     530.804749    29.094709  success  23.039200  0.000005                  \n",
      "28      22.734144   654.039001  success   4.768034  0.000023                  \n",
      "29      67.900581   693.427368  success   8.240181  0.551290                  \n",
      "30     416.218109   113.693657  success  20.401424  0.316958                  \n",
      "31      61.584930   756.186523  success   7.847607  0.343190                  \n",
      "32     418.082092    34.763714  success  20.447056  0.000011                  \n",
      "33      44.284218   331.958740  success   6.654639  0.000055  under-trained   \n",
      "34     452.165710    32.373787  success  21.264189  0.000007                  \n",
      "35      19.978958   728.527222  success   4.469783  0.000114  under-trained   \n",
      "36      61.898289   757.561096  success   7.867547  0.546486                  \n",
      "37     399.234985   118.930992  success  19.980865  0.290297                  \n",
      "38     513.352234    28.375172  success  22.657278  0.000006                  \n",
      "39      46.415596   314.908203  success   6.812899  0.000108                  \n",
      "40     503.679199    28.910372  success  22.442799  0.000002                  \n",
      "41      30.158598   482.085693  success   5.491684  0.000032                  \n",
      "\n",
      "    weak_rank_loss        xmax       xmin  \n",
      "0                0   60.062981  18.636854  \n",
      "1                0   84.952370  11.598700  \n",
      "2                0  479.307434  16.355970  \n",
      "3                0   61.810150  20.930214  \n",
      "4               24  455.737671  11.207310  \n",
      "5                6   54.426411  15.716120  \n",
      "6               21  397.651245  10.295616  \n",
      "7                5   23.148758   5.556564  \n",
      "8                0   88.080826  12.326059  \n",
      "9                0  465.493744  14.097530  \n",
      "10               0   62.914303  20.782856  \n",
      "11              30  459.097290  11.777707  \n",
      "12               6   32.782341   8.540647  \n",
      "13              27  431.755951  14.838481  \n",
      "14               5   20.962111  10.991772  \n",
      "15               0   93.484482  11.928672  \n",
      "16               0  446.999084  13.487392  \n",
      "17               0   65.997604  14.512330  \n",
      "18              30  481.570282  11.244167  \n",
      "19               6   56.604767  12.304750  \n",
      "20              26  469.630981   9.839108  \n",
      "21               5   41.980766  13.609426  \n",
      "22               0   73.939095  12.643573  \n",
      "23               0  431.630402  14.502041  \n",
      "24               0   64.383034  15.464737  \n",
      "25              51  555.487854   9.186387  \n",
      "26               5   49.533775  12.169941  \n",
      "27              43  530.804749  13.646353  \n",
      "28               5   22.734144   7.824432  \n",
      "29               0   67.900581  12.945216  \n",
      "30               0  416.218109  16.336714  \n",
      "31               0   61.584930  14.994084  \n",
      "32              50  418.082092   8.574869  \n",
      "33               6   44.284218  11.422960  \n",
      "34              43  452.165710   9.287139  \n",
      "35               5   19.978958  10.435791  \n",
      "36               0   61.898289  14.039959  \n",
      "37               0  399.234985  19.774689  \n",
      "38              54  513.352234  19.421503  \n",
      "39               6   46.415596   7.940696  \n",
      "40              49  503.679199  19.832781  \n",
      "41               7   30.158598   7.943161  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.23.mlp.up_proj  0.036223  8192  22016  2.6875   \n",
      "1          5     model.layers.24.mlp.down_proj  0.022551  8192  22016  2.6875   \n",
      "2          6     model.layers.24.mlp.gate_proj  0.020886  8192  22016  2.6875   \n",
      "3          7       model.layers.24.mlp.up_proj  0.041619  8192  22016  2.6875   \n",
      "4          9  model.layers.24.self_attn.k_proj  0.019351  8192   8192  1.0000   \n",
      "5         10  model.layers.24.self_attn.o_proj  0.027779  8192   8192  1.0000   \n",
      "6         11  model.layers.24.self_attn.q_proj  0.017158  8192   8192  1.0000   \n",
      "7         12  model.layers.24.self_attn.v_proj  0.033756  8192   8192  1.0000   \n",
      "8         14     model.layers.25.mlp.down_proj  0.016585  8192  22016  2.6875   \n",
      "9         15     model.layers.25.mlp.gate_proj  0.020895  8192  22016  2.6875   \n",
      "10        16       model.layers.25.mlp.up_proj  0.039397  8192  22016  2.6875   \n",
      "11        18  model.layers.25.self_attn.k_proj  0.019200  8192   8192  1.0000   \n",
      "12        19  model.layers.25.self_attn.o_proj  0.031792  8192   8192  1.0000   \n",
      "13        20  model.layers.25.self_attn.q_proj  0.012821  8192   8192  1.0000   \n",
      "14        21  model.layers.25.self_attn.v_proj  0.034049  8192   8192  1.0000   \n",
      "15        23     model.layers.26.mlp.down_proj  0.011823  8192  22016  2.6875   \n",
      "16        24     model.layers.26.mlp.gate_proj  0.024873  8192  22016  2.6875   \n",
      "17        25       model.layers.26.mlp.up_proj  0.025676  8192  22016  2.6875   \n",
      "18        27  model.layers.26.self_attn.k_proj  0.023365  8192   8192  1.0000   \n",
      "19        28  model.layers.26.self_attn.o_proj  0.023146  8192   8192  1.0000   \n",
      "20        29  model.layers.26.self_attn.q_proj  0.022075  8192   8192  1.0000   \n",
      "21        30  model.layers.26.self_attn.v_proj  0.024426  8192   8192  1.0000   \n",
      "22        32     model.layers.27.mlp.down_proj  0.010601  8192  22016  2.6875   \n",
      "23        33     model.layers.27.mlp.gate_proj  0.030028  8192  22016  2.6875   \n",
      "24        34       model.layers.27.mlp.up_proj  0.018050  8192  22016  2.6875   \n",
      "25        36  model.layers.27.self_attn.k_proj  0.027834  8192   8192  1.0000   \n",
      "26        37  model.layers.27.self_attn.o_proj  0.017701  8192   8192  1.0000   \n",
      "27        38  model.layers.27.self_attn.q_proj  0.015487  8192   8192  1.0000   \n",
      "28        39  model.layers.27.self_attn.v_proj  0.024667  8192   8192  1.0000   \n",
      "29        41     model.layers.28.mlp.down_proj  0.009555  8192  22016  2.6875   \n",
      "30        42     model.layers.28.mlp.gate_proj  0.029177  8192  22016  2.6875   \n",
      "31        43       model.layers.28.mlp.up_proj  0.021921  8192  22016  2.6875   \n",
      "32        45  model.layers.28.self_attn.k_proj  0.025523  8192   8192  1.0000   \n",
      "33        46  model.layers.28.self_attn.o_proj  0.031259  8192   8192  1.0000   \n",
      "34        47  model.layers.28.self_attn.q_proj  0.011488  8192   8192  1.0000   \n",
      "35        48  model.layers.28.self_attn.v_proj  0.016577  8192   8192  1.0000   \n",
      "36        49     model.layers.29.mlp.down_proj  0.009950  8192  22016  2.6875   \n",
      "37        50     model.layers.29.mlp.gate_proj  0.024546  8192  22016  2.6875   \n",
      "38        51  model.layers.29.self_attn.k_proj  0.036173  8192   8192  1.0000   \n",
      "39        52  model.layers.29.self_attn.o_proj  0.016994  8192   8192  1.0000   \n",
      "40        53  model.layers.29.self_attn.q_proj  0.020495  8192   8192  1.0000   \n",
      "41        54  model.layers.29.self_attn.v_proj  0.016067  8192   8192  1.0000   \n",
      "\n",
      "        alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0    8.477841       15.078770    4.585370      5483  ...  1.413179   \n",
      "1    4.274982        7.582502    1.736156      5623  ...  0.128852   \n",
      "2    4.525445       11.674429    3.497871      5319  ...  0.294812   \n",
      "3    5.939545       10.497263    2.577168      5515  ...  0.310547   \n",
      "4    3.347580        8.902459   12.049606      2845  ...  0.295767   \n",
      "5    7.515450       10.886461    5.514829      4316  ...  1.005356   \n",
      "6    3.047471        8.243685    6.654214      2855  ...  0.150128   \n",
      "7    7.330188        9.019777    4.202356      4275  ...  0.546845   \n",
      "8    4.233768        7.565079    1.789604      5619  ...  0.132349   \n",
      "9    4.866166       12.465603    3.647810      5345  ...  0.351470   \n",
      "10   6.930572       12.109196    2.917615      5547  ...  0.492507   \n",
      "11   3.386889        9.052788   13.686306      2599  ...  0.291605   \n",
      "12   7.651018       11.942523    3.858946      4377  ...  0.530809   \n",
      "13   3.047294        8.352946    8.242428      2678  ...  0.176859   \n",
      "14  11.451487       13.401656    4.480683      4285  ...  1.214961   \n",
      "15   4.230824        7.557060    1.851464      5618  ...  0.138014   \n",
      "16   5.052437       12.910826    3.534642      5368  ...  0.359596   \n",
      "17   7.234421       12.516334    2.849341      5569  ...  0.517740   \n",
      "18   3.332956        8.666867   12.643772      2803  ...  0.291619   \n",
      "19   5.170571       10.510961    3.810066      4257  ...  0.287114   \n",
      "20   3.055729        8.156177    7.383973      2838  ...  0.164065   \n",
      "21   6.785402        9.153775    4.763716      4230  ...  0.616726   \n",
      "22   4.065917        7.411999    1.773009      5590  ...  0.124237   \n",
      "23   5.139873       13.025516    3.460715      5390  ...  0.363091   \n",
      "24   7.665865       13.217879    2.928660      5593  ...  0.618910   \n",
      "25   2.710444        7.272168    5.787323      2245  ...  0.100441   \n",
      "26   6.742380        9.900326    4.704604      4258  ...  0.608691   \n",
      "27   2.977658        8.318754    9.521332      2322  ...  0.175489   \n",
      "28   6.565619        8.744063    4.128658      4192  ...  0.444185   \n",
      "29   4.177019        8.018154    2.321524      5590  ...  0.178158   \n",
      "30   5.154047       12.939918    3.568372      5403  ...  0.382411   \n",
      "31   7.290745       12.486675    2.794478      5606  ...  0.527908   \n",
      "32   2.820315        7.480549    5.151432      2481  ...  0.101759   \n",
      "33   5.632416        8.173714    4.529689      4215  ...  0.394337   \n",
      "34   2.992650        8.219436    7.593912      2548  ...  0.154196   \n",
      "35   6.848557        9.248312    4.611676      4189  ...  0.576275   \n",
      "36   4.075028        7.877019    1.713758      5600  ...  0.120705   \n",
      "37   5.159589       12.969529    3.581243      5403  ...  0.384554   \n",
      "38   2.613272        6.856192    4.811419      2372  ...  0.085383   \n",
      "39   4.589764        8.113695    3.634952      4146  ...  0.213767   \n",
      "40   2.981699        8.087156    9.560620      2441  ...  0.174479   \n",
      "41   5.609076        8.055348    4.324495      4085  ...  0.346439   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max    sv_min        warning  \\\n",
      "0       60.063366   771.491638  success   7.750056  0.329827  under-trained   \n",
      "1       59.387100   787.017151  success   7.706303  0.545394                  \n",
      "2      379.953766   125.746475  success  19.492403  0.288544                  \n",
      "3       58.526348   789.672363  success   7.650251  0.318791                  \n",
      "4      456.427277    29.620417  success  21.364159  0.000010                  \n",
      "5       28.089497   528.528931  success   5.299953  0.000052  under-trained   \n",
      "6      507.096039    27.105335  success  22.518793  0.000016                  \n",
      "7       17.001898   867.505005  success   4.123336  0.000042  under-trained   \n",
      "8       61.212906   760.056458  success   7.823868  0.546799                  \n",
      "9      364.492615   131.694397  success  19.091690  0.289542                  \n",
      "10      55.874611   824.358643  success   7.474932  0.313442  under-trained   \n",
      "11     470.859650    25.038561  success  21.699301  0.000009                  \n",
      "12      36.383663   413.670776  success   6.031887  0.000071  under-trained   \n",
      "13     550.937744    22.009630  success  23.472063  0.000002                  \n",
      "14      14.801247  1001.358032  success   3.847239  0.000267  under-trained   \n",
      "15      61.121086   759.723389  success   7.817997  0.548356                  \n",
      "16     359.224792   134.202469  success  18.953226  0.304846                  \n",
      "17      53.716637   856.158386  success   7.329164  0.323025  under-trained   \n",
      "18     398.432617    32.538811  success  19.960777  0.000003                  \n",
      "19     107.855759   137.298950  success  10.385363  0.000143                  \n",
      "20     466.812592    28.456930  success  21.605846  0.000011                  \n",
      "21      22.337748   648.272522  success   4.726283  0.000155  under-trained   \n",
      "22      66.520988   696.157532  success   8.156040  0.555889                  \n",
      "23     342.144775   141.455292  success  18.497156  0.293446                  \n",
      "24      52.997028   867.602600  success   7.279906  0.314804  under-trained   \n",
      "25     481.966827    19.702856  success  21.953743  0.000002                  \n",
      "26      29.401703   478.653442  success   5.422334  0.000214  under-trained   \n",
      "27     621.904785    16.313473  success  24.938019  0.000002                  \n",
      "28      21.468201   643.823669  success   4.633379  0.000038  under-trained   \n",
      "29      83.097389   556.347595  success   9.115777  0.597987                  \n",
      "30     324.065247   149.445679  success  18.001812  0.310636                  \n",
      "31      51.602993   888.879089  success   7.183522  0.323010  under-trained   \n",
      "32     449.139069    23.959326  success  21.192902  0.000013                  \n",
      "33      28.261250   509.740906  success   5.316131  0.000075                  \n",
      "34     557.880493    20.153429  success  23.619493  0.000005                  \n",
      "35      22.407991   634.646362  success   4.733708  0.000067  under-trained   \n",
      "36      85.703278   537.948730  success   9.257607  0.617152                  \n",
      "37     326.343475   148.619370  success  18.064980  0.297936                  \n",
      "38     420.343445    25.224216  success  20.502279  0.000005                  \n",
      "39      58.584217   231.029236  success   7.654033  0.000101                  \n",
      "40     515.541992    21.648649  success  22.705549  0.000006                  \n",
      "41      27.297783   484.244415  success   5.224728  0.000010                  \n",
      "\n",
      "    weak_rank_loss        xmax       xmin  \n",
      "0                0   60.063366  28.632980  \n",
      "1                0   59.387100  12.625183  \n",
      "2                0  379.953766  23.179321  \n",
      "3                0   58.526348  17.227226  \n",
      "4               62  456.427277  20.620554  \n",
      "5                6   28.089497  10.827625  \n",
      "6               55  507.096039  11.917222  \n",
      "7                7   17.001898   8.393023  \n",
      "8                0   61.212906  12.870935  \n",
      "9                0  364.492615  24.167364  \n",
      "10               0   55.874611  19.081964  \n",
      "11              83  470.859650  20.328522  \n",
      "12               5   36.383663   7.935045  \n",
      "13              74  550.937744  12.856992  \n",
      "14               5   14.801247   8.938643  \n",
      "15               0   61.121086  13.195347  \n",
      "16               0  359.224792  23.605879  \n",
      "17               0   53.716637  18.670452  \n",
      "18              69  398.432617  20.714024  \n",
      "19               6  107.855759   7.718662  \n",
      "20              60  466.812592  12.700199  \n",
      "21               6   22.337748   9.232264  \n",
      "22               0   66.520988  12.717749  \n",
      "23               0  342.144775  23.261890  \n",
      "24               0   52.997028  19.111868  \n",
      "25             113  481.966827   7.209641  \n",
      "26               7   29.401703   8.872021  \n",
      "27             100  621.904785  12.335150  \n",
      "28               7   21.468201   7.739299  \n",
      "29               0   83.097389  15.792106  \n",
      "30               0  324.065247  23.912836  \n",
      "31               0   51.602993  18.313692  \n",
      "32              86  449.139069   7.339909  \n",
      "33               6   28.261250   8.773393  \n",
      "34              76  557.880493  11.028815  \n",
      "35               6   22.407991   8.801258  \n",
      "36               0   85.703278  12.328978  \n",
      "37               0  326.343475  24.024017  \n",
      "38             107  420.343445   6.788500  \n",
      "39               7   58.584217   6.763339  \n",
      "40              90  515.541992  13.625202  \n",
      "41               6   27.297783   7.718665  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.29.mlp.up_proj  0.014968  8192  22016  2.6875   \n",
      "1          5     model.layers.30.mlp.down_proj  0.007038  8192  22016  2.6875   \n",
      "2          6     model.layers.30.mlp.gate_proj  0.034839  8192  22016  2.6875   \n",
      "3          7       model.layers.30.mlp.up_proj  0.014339  8192  22016  2.6875   \n",
      "4          9  model.layers.30.self_attn.k_proj  0.023988  8192   8192  1.0000   \n",
      "5         10  model.layers.30.self_attn.o_proj  0.033364  8192   8192  1.0000   \n",
      "6         11  model.layers.30.self_attn.q_proj  0.030132  8192   8192  1.0000   \n",
      "7         12  model.layers.30.self_attn.v_proj  0.030732  8192   8192  1.0000   \n",
      "8         14     model.layers.31.mlp.down_proj  0.010666  8192  22016  2.6875   \n",
      "9         15     model.layers.31.mlp.gate_proj  0.028888  8192  22016  2.6875   \n",
      "10        16       model.layers.31.mlp.up_proj  0.018416  8192  22016  2.6875   \n",
      "11        18  model.layers.31.self_attn.k_proj  0.016479  8192   8192  1.0000   \n",
      "12        19  model.layers.31.self_attn.o_proj  0.028695  8192   8192  1.0000   \n",
      "13        20  model.layers.31.self_attn.q_proj  0.031619  8192   8192  1.0000   \n",
      "14        21  model.layers.31.self_attn.v_proj  0.019911  8192   8192  1.0000   \n",
      "15        23     model.layers.32.mlp.down_proj  0.007459  8192  22016  2.6875   \n",
      "16        24     model.layers.32.mlp.gate_proj  0.032945  8192  22016  2.6875   \n",
      "17        25       model.layers.32.mlp.up_proj  0.021625  8192  22016  2.6875   \n",
      "18        27  model.layers.32.self_attn.k_proj  0.022769  8192   8192  1.0000   \n",
      "19        28  model.layers.32.self_attn.o_proj  0.032165  8192   8192  1.0000   \n",
      "20        29  model.layers.32.self_attn.q_proj  0.031614  8192   8192  1.0000   \n",
      "21        30  model.layers.32.self_attn.v_proj  0.043554  8192   8192  1.0000   \n",
      "22        32     model.layers.33.mlp.down_proj  0.008949  8192  22016  2.6875   \n",
      "23        33     model.layers.33.mlp.gate_proj  0.031101  8192  22016  2.6875   \n",
      "24        34       model.layers.33.mlp.up_proj  0.023579  8192  22016  2.6875   \n",
      "25        36  model.layers.33.self_attn.k_proj  0.012163  8192   8192  1.0000   \n",
      "26        37  model.layers.33.self_attn.o_proj  0.035664  8192   8192  1.0000   \n",
      "27        38  model.layers.33.self_attn.q_proj  0.033151  8192   8192  1.0000   \n",
      "28        39  model.layers.33.self_attn.v_proj  0.056995  8192   8192  1.0000   \n",
      "29        41     model.layers.34.mlp.down_proj  0.013442  8192  22016  2.6875   \n",
      "30        42     model.layers.34.mlp.gate_proj  0.028718  8192  22016  2.6875   \n",
      "31        43       model.layers.34.mlp.up_proj  0.021461  8192  22016  2.6875   \n",
      "32        45  model.layers.34.self_attn.k_proj  0.011261  8192   8192  1.0000   \n",
      "33        46  model.layers.34.self_attn.o_proj  0.013690  8192   8192  1.0000   \n",
      "34        47  model.layers.34.self_attn.q_proj  0.026644  8192   8192  1.0000   \n",
      "35        48  model.layers.34.self_attn.v_proj  0.030940  8192   8192  1.0000   \n",
      "36        49     model.layers.35.mlp.down_proj  0.016276  8192  22016  2.6875   \n",
      "37        50     model.layers.35.mlp.gate_proj  0.028326  8192  22016  2.6875   \n",
      "38        51  model.layers.35.self_attn.k_proj  0.010011  8192   8192  1.0000   \n",
      "39        52  model.layers.35.self_attn.o_proj  0.019449  8192   8192  1.0000   \n",
      "40        53  model.layers.35.self_attn.q_proj  0.021879  8192   8192  1.0000   \n",
      "41        54  model.layers.35.self_attn.v_proj  0.018749  8192   8192  1.0000   \n",
      "\n",
      "       alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   7.334899       12.572189    2.981501      5616  ...  0.653395   \n",
      "1   4.078904        7.830580    1.694921      5612  ...  0.120211   \n",
      "2   5.065977       12.656833    3.444194      5399  ...  0.352565   \n",
      "3   7.037304       12.202195    2.588099      5613  ...  0.435705   \n",
      "4   3.599790        9.117124   13.697644      2896  ...  0.387554   \n",
      "5   3.757019        7.288662    3.332457      4022  ...  0.139966   \n",
      "6   3.753888        9.691985   16.503769      2858  ...  0.452736   \n",
      "7   6.208963        8.893139    6.603176      4038  ...  0.672474   \n",
      "8   4.261907        8.151981    1.668310      5650  ...  0.124722   \n",
      "9   4.970685       12.453949    3.722079      5387  ...  0.378590   \n",
      "10  6.540924       11.344654    2.515386      5606  ...  0.372723   \n",
      "11  2.696742        6.987469    3.914337      2873  ...  0.084626   \n",
      "12  4.284629        6.485888    2.992042      4175  ...  0.152980   \n",
      "13  3.066005        7.990185    6.975645      2886  ...  0.151487   \n",
      "14  5.062925        7.694982    4.093381      4111  ...  0.271466   \n",
      "15  4.469472        8.510601    1.691969      5699  ...  0.136610   \n",
      "16  3.870564        9.875311    1.671354      5364  ...  0.103448   \n",
      "17  7.282539       12.633585    2.695011      5602  ...  0.484708   \n",
      "18  2.593316        6.589590    3.819089      2881  ...  0.079766   \n",
      "19  3.901358        6.364115    2.731831      4195  ...  0.124970   \n",
      "20  3.782819        9.711326   18.864393      2898  ...  0.491938   \n",
      "21  3.921278        6.590934    3.077847      4091  ...  0.135326   \n",
      "22  4.584089        8.821189    1.629391      5728  ...  0.135660   \n",
      "23  4.921602       12.630344    3.984623      5353  ...  0.396142   \n",
      "24  7.263242       12.491857    2.707919      5590  ...  0.476186   \n",
      "25  2.622195        6.697504    4.437401      2910  ...  0.091400   \n",
      "26  4.760845        6.891322    3.898412      4178  ...  0.243779   \n",
      "27  2.677155        6.893105    3.784197      2948  ...  0.082032   \n",
      "28  3.699782        5.068564    2.748258      4049  ...  0.110218   \n",
      "29  4.823439        8.959280    1.594511      5772  ...  0.141512   \n",
      "30  3.682182        9.481365    1.597189      5321  ...  0.092988   \n",
      "31  7.275937       12.442840    2.964392      5567  ...  0.559105   \n",
      "32  2.584869        6.731087    3.319078      2969  ...  0.074795   \n",
      "33  6.587745       10.515922    3.941417      4308  ...  0.439015   \n",
      "34  2.775357        7.189610    4.330003      3039  ...  0.096424   \n",
      "35  8.958805       12.714697    6.549415      4090  ...  1.186429   \n",
      "36  5.093387        9.695699    1.599357      5817  ...  0.153514   \n",
      "37  3.544645        9.246220    1.504798      5280  ...  0.083894   \n",
      "38  2.557984        6.701053    3.130144      3008  ...  0.071941   \n",
      "39  5.200202        7.674532    3.372135      4270  ...  0.238555   \n",
      "40  2.644371        6.887759    3.763124      3062  ...  0.083480   \n",
      "41  6.030812        9.851877    4.801750      4078  ...  0.420698   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max    sv_min        warning  \\\n",
      "0       51.763458   883.026855  success   7.194683  0.317026  under-trained   \n",
      "1       83.133331   554.749573  success   9.117748  0.649361                  \n",
      "2      315.064392   153.767822  success  17.750053  0.318761                  \n",
      "3       54.191406   843.371277  success   7.361481  0.342995  under-trained   \n",
      "4      340.943207    39.215614  success  18.464647  0.000004                  \n",
      "5       87.098755   157.612259  success   9.332671  0.000080                  \n",
      "6      381.814545    36.407967  success  19.540075  0.000009                  \n",
      "7       27.058687   504.998688  success   5.201797  0.000021  under-trained   \n",
      "8       81.800171   563.692749  success   9.044345  0.699892                  \n",
      "9      320.242859   151.301987  success  17.895330  0.346513                  \n",
      "10      54.251461   840.368042  success   7.365559  0.365125  under-trained   \n",
      "11     390.011810    38.375851  success  19.748716  0.000010                  \n",
      "12      32.640545   442.515411  success   5.713191  0.000090                  \n",
      "13     403.699158    37.646358  success  20.092266  0.000013                  \n",
      "14      33.103119   431.507996  success   5.753531  0.000115                  \n",
      "15      80.197762   573.559875  success   8.955320  0.684098                  \n",
      "16     355.949463   136.456787  success  18.866623  0.338945                  \n",
      "17      54.297215   834.583313  success   7.368664  0.352446  under-trained   \n",
      "18     347.527740    39.154099  success  18.642096  0.000009                  \n",
      "19      42.781506   356.663483  success   6.540757  0.000028                  \n",
      "20     369.163971    37.751755  success  19.213640  0.000002                  \n",
      "21      47.952644   315.577698  success   6.924785  0.000006                  \n",
      "22      84.005142   547.253906  success   9.165432  0.675405                  \n",
      "23     368.389648   132.413101  success  19.193480  0.361488                  \n",
      "24      52.465443   862.396362  success   7.243303  0.369762  under-trained   \n",
      "25     358.228088    39.403885  success  18.926914  0.000014                  \n",
      "26      28.022036   543.464478  success   5.293584  0.000128                  \n",
      "27     375.653870    38.343048  success  19.381792  0.000012                  \n",
      "28      23.440296   644.641418  success   4.841518  0.000009                  \n",
      "29      72.018867   638.642883  success   8.486393  0.683824                  \n",
      "30     375.777802   130.372955  success  19.384989  0.374615                  \n",
      "31      51.302162   879.449646  success   7.162553  0.395553  under-trained   \n",
      "32     401.822662    37.279415  success  20.045515  0.000002                  \n",
      "33      39.471672   400.072937  success   6.282649  0.000094  under-trained   \n",
      "34     389.508484    38.882946  success  19.735970  0.000006                  \n",
      "35      26.256706   597.909241  success   5.124130  0.000045  under-trained   \n",
      "36      80.091347   572.239258  success   8.949377  0.660972                  \n",
      "37     405.979065   121.992126  success  20.148922  0.364226                  \n",
      "38     416.545197    36.738110  success  20.409439  0.000006                  \n",
      "39      29.909849   541.041504  success   5.468990  0.000079                  \n",
      "40     402.426910    38.278999  success  20.060581  0.000004                  \n",
      "41      43.012089   372.129120  success   6.558360  0.000039  under-trained   \n",
      "\n",
      "    weak_rank_loss        xmax       xmin  \n",
      "0                0   51.763458  19.294203  \n",
      "1                0   83.133331  12.226718  \n",
      "2                0  315.064392  23.186567  \n",
      "3                0   54.191406  17.096184  \n",
      "4               63  340.943207  23.086388  \n",
      "5                5   87.098755   6.353516  \n",
      "6               57  381.814545  28.767696  \n",
      "7                8   27.058687  11.780253  \n",
      "8                0   81.800171  12.072270  \n",
      "9                0  320.242859  24.833494  \n",
      "10               0   54.251461  16.650820  \n",
      "11              59  390.011810   7.971297  \n",
      "12               6   32.640545   6.086156  \n",
      "13              51  403.699158  13.776412  \n",
      "14               5   33.103119   7.938849  \n",
      "15               0   80.197762  12.177669  \n",
      "16               0  355.949463  12.738173  \n",
      "17               0   54.297215  17.542173  \n",
      "18              56  347.527740   7.093422  \n",
      "19               6   42.781506   5.946935  \n",
      "20              49  369.163971  32.861275  \n",
      "21               8   47.952644   6.536217  \n",
      "22               0   84.005142  11.817978  \n",
      "23               0  368.389648  26.564501  \n",
      "24               0   52.465443  17.589390  \n",
      "25              42  358.228088   8.426265  \n",
      "26               6   28.022036   8.102518  \n",
      "27              36  375.653870   7.450198  \n",
      "28               6   23.440296   5.917924  \n",
      "29               0   72.018867  11.628143  \n",
      "30               0  375.777802  12.401720  \n",
      "31               0   51.302162  18.949976  \n",
      "32              45  401.822662   6.896705  \n",
      "33               5   39.471672   8.484800  \n",
      "34              39  389.508484   8.843001  \n",
      "35               5   26.256706  13.432568  \n",
      "36               0   80.091347  11.613806  \n",
      "37               0  405.979065  11.978623  \n",
      "38              41  416.545197   6.695794  \n",
      "39               4   29.909849   7.569261  \n",
      "40              36  402.426910   7.932413  \n",
      "41               6   43.012089  10.280146  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.35.mlp.up_proj  0.022383  8192  22016  2.6875   \n",
      "1          5     model.layers.36.mlp.down_proj  0.024131  8192  22016  2.6875   \n",
      "2          6     model.layers.36.mlp.gate_proj  0.020789  8192  22016  2.6875   \n",
      "3          7       model.layers.36.mlp.up_proj  0.017218  8192  22016  2.6875   \n",
      "4          9  model.layers.36.self_attn.k_proj  0.021255  8192   8192  1.0000   \n",
      "5         10  model.layers.36.self_attn.o_proj  0.025887  8192   8192  1.0000   \n",
      "6         11  model.layers.36.self_attn.q_proj  0.025753  8192   8192  1.0000   \n",
      "7         12  model.layers.36.self_attn.v_proj  0.050520  8192   8192  1.0000   \n",
      "8         14     model.layers.37.mlp.down_proj  0.026852  8192  22016  2.6875   \n",
      "9         15     model.layers.37.mlp.gate_proj  0.023396  8192  22016  2.6875   \n",
      "10        16       model.layers.37.mlp.up_proj  0.021348  8192  22016  2.6875   \n",
      "11        18  model.layers.37.self_attn.k_proj  0.009498  8192   8192  1.0000   \n",
      "12        19  model.layers.37.self_attn.o_proj  0.022523  8192   8192  1.0000   \n",
      "13        20  model.layers.37.self_attn.q_proj  0.016532  8192   8192  1.0000   \n",
      "14        21  model.layers.37.self_attn.v_proj  0.038819  8192   8192  1.0000   \n",
      "15        23     model.layers.38.mlp.down_proj  0.026378  8192  22016  2.6875   \n",
      "16        24     model.layers.38.mlp.gate_proj  0.023019  8192  22016  2.6875   \n",
      "17        25       model.layers.38.mlp.up_proj  0.018798  8192  22016  2.6875   \n",
      "18        27  model.layers.38.self_attn.k_proj  0.010882  8192   8192  1.0000   \n",
      "19        28  model.layers.38.self_attn.o_proj  0.036214  8192   8192  1.0000   \n",
      "20        29  model.layers.38.self_attn.q_proj  0.020767  8192   8192  1.0000   \n",
      "21        30  model.layers.38.self_attn.v_proj  0.050968  8192   8192  1.0000   \n",
      "22        32     model.layers.39.mlp.down_proj  0.025523  8192  22016  2.6875   \n",
      "23        33     model.layers.39.mlp.gate_proj  0.024832  8192  22016  2.6875   \n",
      "24        34       model.layers.39.mlp.up_proj  0.016996  8192  22016  2.6875   \n",
      "25        36  model.layers.39.self_attn.k_proj  0.013984  8192   8192  1.0000   \n",
      "26        37  model.layers.39.self_attn.o_proj  0.025684  8192   8192  1.0000   \n",
      "27        38  model.layers.39.self_attn.q_proj  0.015664  8192   8192  1.0000   \n",
      "28        39  model.layers.39.self_attn.v_proj  0.043736  8192   8192  1.0000   \n",
      "29        41     model.layers.40.mlp.down_proj  0.024878  8192  22016  2.6875   \n",
      "30        42     model.layers.40.mlp.gate_proj  0.024956  8192  22016  2.6875   \n",
      "31        43       model.layers.40.mlp.up_proj  0.018977  8192  22016  2.6875   \n",
      "32        45  model.layers.40.self_attn.k_proj  0.011334  8192   8192  1.0000   \n",
      "33        46  model.layers.40.self_attn.o_proj  0.036200  8192   8192  1.0000   \n",
      "34        47  model.layers.40.self_attn.q_proj  0.012698  8192   8192  1.0000   \n",
      "35        48  model.layers.40.self_attn.v_proj  0.064919  8192   8192  1.0000   \n",
      "36        49     model.layers.41.mlp.down_proj  0.024818  8192  22016  2.6875   \n",
      "37        50     model.layers.41.mlp.gate_proj  0.027148  8192  22016  2.6875   \n",
      "38        51  model.layers.41.self_attn.k_proj  0.012582  8192   8192  1.0000   \n",
      "39        52  model.layers.41.self_attn.o_proj  0.048598  8192   8192  1.0000   \n",
      "40        53  model.layers.41.self_attn.q_proj  0.013813  8192   8192  1.0000   \n",
      "41        54  model.layers.41.self_attn.v_proj  0.040560  8192   8192  1.0000   \n",
      "\n",
      "        alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0    7.782824       13.532290    3.179412      5544  ...  0.674916   \n",
      "1    5.515544       10.370567    1.588670      5867  ...  0.168872   \n",
      "2    4.659344       12.242495    4.903611      5241  ...  0.419756   \n",
      "3    7.414329       12.857536    3.280332      5534  ...  0.665135   \n",
      "4    2.590302        6.781670    3.725820      2893  ...  0.079317   \n",
      "5    6.485467       12.123963    3.421371      4324  ...  0.332605   \n",
      "6    2.567567        6.731512    3.192471      2959  ...  0.070960   \n",
      "7    8.409870       13.085661    4.826698      4140  ...  0.697062   \n",
      "8    6.202876       11.442067    1.673990      5905  ...  0.211527   \n",
      "9    3.353231        8.890667    1.420902      5206  ...  0.074828   \n",
      "10   7.690837       13.307337    3.281915      5528  ...  0.675877   \n",
      "11   2.680897        7.171763    3.956093      2943  ...  0.088224   \n",
      "12   6.271250        9.638619    3.901245      4298  ...  0.387550   \n",
      "13   2.728819        7.266453    3.951823      3020  ...  0.089999   \n",
      "14   9.732058       12.557696    5.393524      4107  ...  1.036305   \n",
      "15   6.671779       12.190663    1.720995      5924  ...  0.242730   \n",
      "16   3.308254        8.870403    1.402920      5187  ...  0.072884   \n",
      "17   7.241808       12.350291    3.233036      5532  ...  0.609138   \n",
      "18   2.675217        7.006129    3.193874      3068  ...  0.076703   \n",
      "19   7.092012       14.175758    3.229821      4413  ...  0.359600   \n",
      "20   2.720601        7.130387    3.609801      3122  ...  0.085182   \n",
      "21  10.374659       13.522321    6.397273      4133  ...  1.446541   \n",
      "22   7.037915       12.857905    1.767072      5932  ...  0.272487   \n",
      "23   3.278086        8.832074    1.345358      5185  ...  0.069741   \n",
      "24   7.636851       12.782976    3.231843      5540  ...  0.663685   \n",
      "25   2.582158        6.768744    3.441918      2867  ...  0.075598   \n",
      "26   7.994108       12.519234    3.896180      4400  ...  0.604199   \n",
      "27   2.653006        7.012027    3.897221      2971  ...  0.085820   \n",
      "28   7.177689        9.112679    5.163831      4109  ...  0.600030   \n",
      "29   7.319632       13.532991    1.787326      5935  ...  0.292125   \n",
      "30   4.321969       11.751423    4.490261      5172  ...  0.316737   \n",
      "31   7.927802       13.192359    3.301951      5552  ...  0.738506   \n",
      "32   2.485961        6.575681    2.627135      2817  ...  0.061542   \n",
      "33   7.490672       15.253478    3.263956      4429  ...  0.398719   \n",
      "34   2.481771        6.603148    2.446506      2913  ...  0.059271   \n",
      "35  10.205351       14.145444    6.128783      4145  ...  1.315050   \n",
      "36   7.435371       13.737016    1.804222      5931  ...  0.302360   \n",
      "37   3.260276        8.930576    1.342473      5181  ...  0.069099   \n",
      "38   2.557438        6.830849    3.077970      2798  ...  0.069237   \n",
      "39   6.629166       14.043828    2.914708      4443  ...  0.285044   \n",
      "40   2.550772        6.860244    2.773413      2913  ...  0.065827   \n",
      "41   9.957092       13.114578    5.886966      4180  ...  1.398863   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max    sv_min        warning  \\\n",
      "0       54.794609   817.770630  success   7.402338  0.393481  under-trained   \n",
      "1       75.900337   602.599854  success   8.712080  0.666320                  \n",
      "2      424.145599   118.270554  success  20.594795  0.377535                  \n",
      "3       54.218418   821.223022  success   7.363316  0.415402  under-trained   \n",
      "4      415.049774    33.582718  success  20.372770  0.000006                  \n",
      "5       74.029488   214.175705  success   8.604039  0.000038  under-trained   \n",
      "6      418.549683    33.877316  success  20.458487  0.000023                  \n",
      "7       35.973988   431.447998  success   5.997832  0.000046  under-trained   \n",
      "8       69.926064   651.365479  success   8.362181  0.682453  under-trained   \n",
      "9      448.098572   113.626518  success  21.168339  0.410550                  \n",
      "10      53.738377   820.908386  success   7.330647  0.444295  under-trained   \n",
      "11     473.298401    30.011148  success  21.755423  0.000022                  \n",
      "12      34.431309   473.446442  success   5.867820  0.000001  under-trained   \n",
      "13     460.104340    31.174379  success  21.450043  0.000022                  \n",
      "14      19.513865   818.372803  success   4.417450  0.000004  under-trained   \n",
      "15      67.173561   676.016052  success   8.195948  0.668969  under-trained   \n",
      "16     480.058716   107.499283  success  21.910242  0.410177                  \n",
      "17      50.747601   864.887268  success   7.123735  0.454856  under-trained   \n",
      "18     415.816101    34.219387  success  20.391569  0.000009                  \n",
      "19      99.731972   165.386414  success   9.986589  0.000132  under-trained   \n",
      "20     417.721252    34.644165  success  20.438231  0.000012                  \n",
      "21      20.109396   800.814392  success   4.484350  0.000076  under-trained   \n",
      "22      67.134827   674.332153  success   8.193584  0.660016  under-trained   \n",
      "23     494.627167   105.684502  success  22.240215  0.422741                  \n",
      "24      47.190456   926.663391  success   6.869531  0.469843  under-trained   \n",
      "25     418.168427    29.560858  success  20.449167  0.000001                  \n",
      "26      36.817776   441.807098  success   6.067765  0.000093  under-trained   \n",
      "27     439.592407    29.249510  success  20.966459  0.000010                  \n",
      "28      18.603048   843.220215  success   4.313125  0.000027  under-trained   \n",
      "29      70.609367   638.920593  success   8.402938  0.653232  under-trained   \n",
      "30     523.597290   101.269539  success  22.882248  0.439537                  \n",
      "31      46.138401   942.843811  success   6.792525  0.481764  under-trained   \n",
      "32     441.698334    28.080830  success  21.016621  0.000006                  \n",
      "33     108.725098   147.778503  success  10.427133  0.000159  under-trained   \n",
      "34     457.782532    28.103445  success  21.395853  0.000005                  \n",
      "35      24.326582   635.886780  success   4.932199  0.000025  under-trained   \n",
      "36      70.391869   638.600952  success   8.389986  0.640214  under-trained   \n",
      "37     548.540527    97.704163  success  23.420942  0.463141                  \n",
      "38     468.784912    24.928047  success  21.651442  0.000007                  \n",
      "39     131.368408   121.960518  success  11.461606  0.000068  under-trained   \n",
      "40     489.190063    25.033054  success  22.117641  0.000019                  \n",
      "41      20.754362   741.119080  success   4.555696  0.000042  under-trained   \n",
      "\n",
      "    weak_rank_loss        xmax       xmin  \n",
      "0                0   54.794609  19.996294  \n",
      "1                0   75.900337  11.529881  \n",
      "2                0  424.145599  32.946339  \n",
      "3                0   54.218418  20.418930  \n",
      "4               55  415.049774   7.104665  \n",
      "5                4   74.029488   7.510258  \n",
      "6               49  418.549683   6.308017  \n",
      "7                6   35.973988  10.015547  \n",
      "8                0   69.926064  11.956420  \n",
      "9                0  448.098572  11.795420  \n",
      "10               0   53.738377  20.239101  \n",
      "11              39  473.298401   7.641522  \n",
      "12               6   34.431309   8.677915  \n",
      "13              30  460.104340   7.712400  \n",
      "14               7   19.513865  11.408227  \n",
      "15               0   67.173561  12.179486  \n",
      "16               0  480.058716  11.842077  \n",
      "17               0   50.747601  19.874544  \n",
      "18              32  415.816101   6.333981  \n",
      "19               5   99.731972   7.428989  \n",
      "20              28  417.721252   7.180415  \n",
      "21               5   20.109396  13.481323  \n",
      "22               0   67.134827  12.396976  \n",
      "23               0  494.627167  11.627669  \n",
      "24               0   47.190456  19.796061  \n",
      "25              45  418.168427   5.873127  \n",
      "26               4   36.817776   8.648690  \n",
      "27              41  439.592407   6.827424  \n",
      "28               5   18.603048  10.768745  \n",
      "29               0   70.609367  12.464672  \n",
      "30               0  523.597290  32.150524  \n",
      "31               0   46.138401  20.063932  \n",
      "32              57  441.698334   4.659785  \n",
      "33               3  108.725098   7.303452  \n",
      "34              48  457.782532   4.553575  \n",
      "35               5   24.326582  12.442517  \n",
      "36               0   70.391869  12.513759  \n",
      "37               0  548.540527  11.902175  \n",
      "38              54  468.784912   5.031184  \n",
      "39               7  131.368408   6.598143  \n",
      "40              46  489.190063   4.822340  \n",
      "41               7   20.754362  11.916229  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.41.mlp.up_proj  0.022528  8192  22016  2.6875   \n",
      "1          5     model.layers.42.mlp.down_proj  0.032581  8192  22016  2.6875   \n",
      "2          6     model.layers.42.mlp.gate_proj  0.025390  8192  22016  2.6875   \n",
      "3          7       model.layers.42.mlp.up_proj  0.021668  8192  22016  2.6875   \n",
      "4          9  model.layers.42.self_attn.k_proj  0.017967  8192   8192  1.0000   \n",
      "5         10  model.layers.42.self_attn.o_proj  0.044473  8192   8192  1.0000   \n",
      "6         11  model.layers.42.self_attn.q_proj  0.020427  8192   8192  1.0000   \n",
      "7         12  model.layers.42.self_attn.v_proj  0.041457  8192   8192  1.0000   \n",
      "8         14     model.layers.43.mlp.down_proj  0.030249  8192  22016  2.6875   \n",
      "9         15     model.layers.43.mlp.gate_proj  0.026935  8192  22016  2.6875   \n",
      "10        16       model.layers.43.mlp.up_proj  0.021315  8192  22016  2.6875   \n",
      "11        18  model.layers.43.self_attn.k_proj  0.023443  8192   8192  1.0000   \n",
      "12        19  model.layers.43.self_attn.o_proj  0.047306  8192   8192  1.0000   \n",
      "13        20  model.layers.43.self_attn.q_proj  0.028952  8192   8192  1.0000   \n",
      "14        21  model.layers.43.self_attn.v_proj  0.052441  8192   8192  1.0000   \n",
      "15        23     model.layers.44.mlp.down_proj  0.028777  8192  22016  2.6875   \n",
      "16        24     model.layers.44.mlp.gate_proj  0.028676  8192  22016  2.6875   \n",
      "17        25       model.layers.44.mlp.up_proj  0.034939  8192  22016  2.6875   \n",
      "18        27  model.layers.44.self_attn.k_proj  0.025012  8192   8192  1.0000   \n",
      "19        28  model.layers.44.self_attn.o_proj  0.037681  8192   8192  1.0000   \n",
      "20        29  model.layers.44.self_attn.q_proj  0.030788  8192   8192  1.0000   \n",
      "21        30  model.layers.44.self_attn.v_proj  0.066126  8192   8192  1.0000   \n",
      "22        32     model.layers.45.mlp.down_proj  0.028251  8192  22016  2.6875   \n",
      "23        33     model.layers.45.mlp.gate_proj  0.031067  8192  22016  2.6875   \n",
      "24        34       model.layers.45.mlp.up_proj  0.030258  8192  22016  2.6875   \n",
      "25        36  model.layers.45.self_attn.k_proj  0.023832  8192   8192  1.0000   \n",
      "26        37  model.layers.45.self_attn.o_proj  0.054255  8192   8192  1.0000   \n",
      "27        38  model.layers.45.self_attn.q_proj  0.021622  8192   8192  1.0000   \n",
      "28        39  model.layers.45.self_attn.v_proj  0.063464  8192   8192  1.0000   \n",
      "29        41     model.layers.46.mlp.down_proj  0.023856  8192  22016  2.6875   \n",
      "30        42     model.layers.46.mlp.gate_proj  0.032906  8192  22016  2.6875   \n",
      "31        43       model.layers.46.mlp.up_proj  0.033791  8192  22016  2.6875   \n",
      "32        45  model.layers.46.self_attn.k_proj  0.026609  8192   8192  1.0000   \n",
      "33        46  model.layers.46.self_attn.o_proj  0.033399  8192   8192  1.0000   \n",
      "34        47  model.layers.46.self_attn.q_proj  0.028590  8192   8192  1.0000   \n",
      "35        48  model.layers.46.self_attn.v_proj  0.032121  8192   8192  1.0000   \n",
      "36        49     model.layers.47.mlp.down_proj  0.020514  8192  22016  2.6875   \n",
      "37        50     model.layers.47.mlp.gate_proj  0.034357  8192  22016  2.6875   \n",
      "38        51  model.layers.47.self_attn.k_proj  0.023410  8192   8192  1.0000   \n",
      "39        52  model.layers.47.self_attn.o_proj  0.048892  8192   8192  1.0000   \n",
      "40        53  model.layers.47.self_attn.q_proj  0.026971  8192   8192  1.0000   \n",
      "41        54  model.layers.47.self_attn.v_proj  0.024969  8192   8192  1.0000   \n",
      "\n",
      "        alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0    8.345605       13.976651    3.303350      5570  ...  0.826445   \n",
      "1    8.283446       15.057520    1.879289      5934  ...  0.373633   \n",
      "2    3.250738        8.956824    1.322784      5190  ...  0.068361   \n",
      "3    8.057048       13.360070    3.411650      5586  ...  0.882131   \n",
      "4    2.541998        6.797716    3.959995      2537  ...  0.077196   \n",
      "5    7.083378       15.866042    3.108067      4429  ...  0.345513   \n",
      "6    2.492777        6.710832    3.112904      2639  ...  0.065652   \n",
      "7    9.950248       12.021339    5.498530      4196  ...  1.334224   \n",
      "8    8.270119       14.988458    1.903572      5923  ...  0.378981   \n",
      "9    3.266005        8.954258    1.302300      5212  ...  0.067953   \n",
      "10   8.562161       14.212288    3.102809      5604  ...  0.756216   \n",
      "11   2.267418        6.149965    2.973944      2177  ...  0.056737   \n",
      "12   8.640609       14.739871    3.138444      4503  ...  0.465856   \n",
      "13   2.313963        6.357510    3.187751      2328  ...  0.060544   \n",
      "14  11.667257       15.184063    6.025043      4211  ...  1.885722   \n",
      "15   8.210015       14.629297    1.899749      5914  ...  0.370354   \n",
      "16   3.268886        8.974001    1.235481      5234  ...  0.065334   \n",
      "17   8.618921       14.139405    2.937179      5629  ...  0.689785   \n",
      "18   2.307251        6.316014    3.595697      2034  ...  0.062750   \n",
      "19   6.470358       13.373683    2.921421      4394  ...  0.273860   \n",
      "20   2.321935        6.425646    3.458540      2196  ...  0.062595   \n",
      "21  12.022194       12.795143    5.008604      4193  ...  1.388666   \n",
      "22   8.432436       14.641889    1.963673      5903  ...  0.407908   \n",
      "23   3.316618        9.096985    1.288670      5255  ...  0.068612   \n",
      "24   9.336548       15.236491    2.971068      5645  ...  0.813564   \n",
      "25   2.415995        6.680213    3.723382      2036  ...  0.067200   \n",
      "26   6.345089       12.927504    2.852029      4395  ...  0.254528   \n",
      "27   2.491475        6.931094    3.938664      2189  ...  0.073038   \n",
      "28   7.105402       10.426901    3.775465      4185  ...  0.386140   \n",
      "29   8.501762       14.714860    2.012636      5893  ...  0.432394   \n",
      "30   3.337778        9.080221    1.251923      5283  ...  0.067599   \n",
      "31   9.825254       15.781479    2.888932      5665  ...  0.833908   \n",
      "32   2.284892        6.404700    3.619223      1770  ...  0.061395   \n",
      "33   8.577446       10.831778    3.311368      4479  ...  0.510871   \n",
      "34   2.308155        6.468516    3.656207      1887  ...  0.062721   \n",
      "35  12.888517       14.447585    4.666465      4268  ...  1.547753   \n",
      "36   8.749553       14.504896    2.115689      5881  ...  0.502329   \n",
      "37   3.415840        9.261151    1.353694      5308  ...  0.073958   \n",
      "38   2.343194        6.512528    4.153555      1805  ...  0.068190   \n",
      "39   6.577928       14.416460    2.893814      4424  ...  0.278548   \n",
      "40   2.325143        6.493521    3.858888      1910  ...  0.065604   \n",
      "41   7.889953       10.412537    3.898796      4251  ...  0.514979   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max        sv_min  \\\n",
      "0       47.285919   917.146240  success   6.876476  5.026567e-01   \n",
      "1       65.733162   683.644287  success   8.107599  6.205404e-01   \n",
      "2      569.271973    94.840805  success  23.859421  4.972896e-01   \n",
      "3       45.518112   951.937683  success   6.746711  5.407066e-01   \n",
      "4      472.239563    21.612185  success  21.731073  4.069792e-06   \n",
      "5      173.739151    88.853447  success  13.181015  1.818068e-04   \n",
      "6      492.165161    22.053030  success  22.184795  1.496368e-05   \n",
      "7       16.148966   916.331238  success   4.018578  4.592977e-05   \n",
      "8       64.917679   691.834045  success   8.057151  6.185976e-01   \n",
      "9      551.639160    98.372772  success  23.487000  5.310296e-01   \n",
      "10      45.697765   949.908142  success   6.760012  5.558894e-01   \n",
      "11     515.608948    15.458745  success  22.707024  3.362049e-06   \n",
      "12      50.802303   297.886200  success   7.127573  1.486753e-04   \n",
      "13     559.056763    15.739368  success  23.644381  2.038537e-06   \n",
      "14      20.018215   724.134949  success   4.474172  4.247436e-05   \n",
      "15      60.517956   740.927856  success   7.779329  6.739989e-01   \n",
      "16     556.260254    98.085159  success  23.585171  5.134583e-01   \n",
      "17      43.702629   993.534424  success   6.610796  5.419546e-01   \n",
      "18     546.339600    14.063525  success  23.373909  3.560914e-06   \n",
      "19     116.658195   122.743454  success  10.800842  5.107925e-05   \n",
      "20     585.284668    14.464990  success  24.192657  2.748337e-06   \n",
      "21      11.595608  1168.892090  success   3.405232  2.346359e-05   \n",
      "22      54.497570   822.893127  success   7.382247  6.817170e-01   \n",
      "23     553.158752    98.920807  success  23.519327  5.225161e-01   \n",
      "24      42.846863  1015.159485  success   6.545752  5.455439e-01   \n",
      "25     582.095581    13.452764  success  24.126657  8.845009e-06   \n",
      "26     108.994148   133.983704  success  10.440026  5.716593e-05   \n",
      "27     605.234253    14.082092  success  24.601509  1.762420e-07   \n",
      "28      29.340059   473.941803  success   5.416646  2.780929e-05   \n",
      "29      53.802338   834.979492  success   7.335008  6.784167e-01   \n",
      "30     525.338135   104.100876  success  22.920256  5.206009e-01   \n",
      "31      40.384605  1081.710571  success   6.354888  5.514970e-01   \n",
      "32     635.425171    10.903403  success  25.207642  6.030974e-06   \n",
      "33      18.315590   810.276917  success   4.279672  1.341401e-04   \n",
      "34     634.543335    11.933475  success  25.190144  1.885123e-06   \n",
      "35      13.211913  1077.397339  success   3.634820  8.270998e-05   \n",
      "36      45.476501   989.491821  success   6.743627  6.713941e-01   \n",
      "37     514.323853   106.344643  success  22.678709  5.082833e-01   \n",
      "38     601.642212    11.012757  success  24.528397  4.777645e-08   \n",
      "39     155.468109    94.972084  success  12.468685  4.637280e-05   \n",
      "40     620.497437    12.029125  success  24.909786  2.945104e-06   \n",
      "41      20.879545   674.182922  success   4.569414  1.130993e-04   \n",
      "\n",
      "          warning  weak_rank_loss        xmax       xmin  \n",
      "0   under-trained               0   47.285919  20.010157  \n",
      "1   under-trained               0   65.733162  12.920279  \n",
      "2                               0  569.271973  11.860523  \n",
      "3   under-trained               0   45.518112  20.566103  \n",
      "4                              94  472.239563   5.485981  \n",
      "5   under-trained               5  173.739151   6.723943  \n",
      "6                              81  492.165161   4.716657  \n",
      "7   under-trained               6   16.148966  10.763253  \n",
      "8   under-trained               0   64.917679  13.045181  \n",
      "9                               0  551.639160  11.787867  \n",
      "10  under-trained               0   45.697765  18.965721  \n",
      "11                            126  515.608948   3.318753  \n",
      "12  under-trained               6   50.802303   6.646214  \n",
      "13                            111  559.056763   3.900177  \n",
      "14  under-trained               5   20.018215  11.474691  \n",
      "15  under-trained               0   60.517956  13.004435  \n",
      "16                              0  556.260254  11.404659  \n",
      "17  under-trained               0   43.702629  18.093519  \n",
      "18                            140  546.339600   3.778921  \n",
      "19  under-trained               8  116.658195   5.908704  \n",
      "20                            126  585.284668   4.027427  \n",
      "21  under-trained               8   11.595608   9.044742  \n",
      "22  under-trained               0   54.497570  13.356266  \n",
      "23                              0  553.158752  11.792759  \n",
      "24  under-trained               0   42.846863  18.305470  \n",
      "25                            134  582.095581   3.968302  \n",
      "26  under-trained               7  108.994148   5.900851  \n",
      "27                            120  605.234253   4.550229  \n",
      "28  under-trained               8   29.340059   7.186899  \n",
      "29  under-trained               0   53.802338  13.645150  \n",
      "30                              0  525.338135  11.543165  \n",
      "31  under-trained               0   40.384605  17.948174  \n",
      "32                            171  635.425171   3.412195  \n",
      "33  under-trained               4   18.315590   6.830767  \n",
      "34                            153  634.543335   3.770723  \n",
      "35  under-trained               6   13.211913   8.906324  \n",
      "36  under-trained               0   45.476501  14.236568  \n",
      "37                              0  514.323853  12.223924  \n",
      "38                            174  601.642212   3.696185  \n",
      "39  under-trained               7  155.468109   6.042520  \n",
      "40                            159  620.497437   3.904840  \n",
      "41  under-trained               8   20.879545   7.487242  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.47.mlp.up_proj  0.040970  8192  22016  2.6875   \n",
      "1          5     model.layers.48.mlp.down_proj  0.020795  8192  22016  2.6875   \n",
      "2          6     model.layers.48.mlp.gate_proj  0.035571  8192  22016  2.6875   \n",
      "3          7       model.layers.48.mlp.up_proj  0.044274  8192  22016  2.6875   \n",
      "4          9  model.layers.48.self_attn.k_proj  0.026456  8192   8192  1.0000   \n",
      "5         10  model.layers.48.self_attn.o_proj  0.027141  8192   8192  1.0000   \n",
      "6         11  model.layers.48.self_attn.q_proj  0.026249  8192   8192  1.0000   \n",
      "7         12  model.layers.48.self_attn.v_proj  0.025196  8192   8192  1.0000   \n",
      "8         14     model.layers.49.mlp.down_proj  0.019818  8192  22016  2.6875   \n",
      "9         15     model.layers.49.mlp.gate_proj  0.037655  8192  22016  2.6875   \n",
      "10        16       model.layers.49.mlp.up_proj  0.043156  8192  22016  2.6875   \n",
      "11        18  model.layers.49.self_attn.k_proj  0.030054  8192   8192  1.0000   \n",
      "12        19  model.layers.49.self_attn.o_proj  0.042458  8192   8192  1.0000   \n",
      "13        20  model.layers.49.self_attn.q_proj  0.036580  8192   8192  1.0000   \n",
      "14        21  model.layers.49.self_attn.v_proj  0.034782  8192   8192  1.0000   \n",
      "15        23     model.layers.50.mlp.down_proj  0.016026  8192  22016  2.6875   \n",
      "16        24     model.layers.50.mlp.gate_proj  0.038242  8192  22016  2.6875   \n",
      "17        25       model.layers.50.mlp.up_proj  0.038059  8192  22016  2.6875   \n",
      "18        27  model.layers.50.self_attn.k_proj  0.028149  8192   8192  1.0000   \n",
      "19        28  model.layers.50.self_attn.o_proj  0.033899  8192   8192  1.0000   \n",
      "20        29  model.layers.50.self_attn.q_proj  0.035559  8192   8192  1.0000   \n",
      "21        30  model.layers.50.self_attn.v_proj  0.051547  8192   8192  1.0000   \n",
      "22        32     model.layers.51.mlp.down_proj  0.013449  8192  22016  2.6875   \n",
      "23        33     model.layers.51.mlp.gate_proj  0.041329  8192  22016  2.6875   \n",
      "24        34       model.layers.51.mlp.up_proj  0.043640  8192  22016  2.6875   \n",
      "25        36  model.layers.51.self_attn.k_proj  0.033363  8192   8192  1.0000   \n",
      "26        37  model.layers.51.self_attn.o_proj  0.043150  8192   8192  1.0000   \n",
      "27        38  model.layers.51.self_attn.q_proj  0.042259  8192   8192  1.0000   \n",
      "28        39  model.layers.51.self_attn.v_proj  0.034881  8192   8192  1.0000   \n",
      "29        41     model.layers.52.mlp.down_proj  0.019211  8192  22016  2.6875   \n",
      "30        42     model.layers.52.mlp.gate_proj  0.043746  8192  22016  2.6875   \n",
      "31        43       model.layers.52.mlp.up_proj  0.039196  8192  22016  2.6875   \n",
      "32        45  model.layers.52.self_attn.k_proj  0.031401  8192   8192  1.0000   \n",
      "33        46  model.layers.52.self_attn.o_proj  0.028746  8192   8192  1.0000   \n",
      "34        47  model.layers.52.self_attn.q_proj  0.042737  8192   8192  1.0000   \n",
      "35        48  model.layers.52.self_attn.v_proj  0.029561  8192   8192  1.0000   \n",
      "36        49     model.layers.53.mlp.down_proj  0.019208  8192  22016  2.6875   \n",
      "37        50     model.layers.53.mlp.gate_proj  0.045861  8192  22016  2.6875   \n",
      "38        51  model.layers.53.self_attn.k_proj  0.030460  8192   8192  1.0000   \n",
      "39        52  model.layers.53.self_attn.o_proj  0.032257  8192   8192  1.0000   \n",
      "40        53  model.layers.53.self_attn.q_proj  0.041566  8192   8192  1.0000   \n",
      "41        54  model.layers.53.self_attn.v_proj  0.015501  8192   8192  1.0000   \n",
      "\n",
      "        alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   10.006341       16.067046    2.843040      5682  ...  0.851019   \n",
      "1    8.602344       14.127317    2.096262      5867  ...  0.469674   \n",
      "2    3.478258        9.382429    1.375358      5336  ...  0.076590   \n",
      "3   10.062739       15.966819    2.731441      5698  ...  0.785839   \n",
      "4    2.332023        6.471561    4.312378      1754  ...  0.068694   \n",
      "5    7.975359       12.883106    3.370948      4465  ...  0.477944   \n",
      "6    2.308410        6.446243    3.960018      1853  ...  0.065585   \n",
      "7   12.161959       13.564567    4.370259      4299  ...  1.272023   \n",
      "8    8.356638       13.447147    2.114504      5855  ...  0.454495   \n",
      "9    3.504731        9.431687    1.351025      5361  ...  0.076217   \n",
      "10  10.542492       16.647971    2.725174      5716  ...  0.860418   \n",
      "11   2.109996        6.019774    4.030385      1427  ...  0.059934   \n",
      "12   8.060348       15.839852    3.165358      4441  ...  0.418955   \n",
      "13   2.096160        5.928748    3.599814      1540  ...  0.056011   \n",
      "14  13.402017       15.094877    4.516933      4280  ...  1.657289   \n",
      "15   8.259344       13.038734    2.203194      5838  ...  0.486122   \n",
      "16   3.515274        9.421787    1.284038      5384  ...  0.073441   \n",
      "17  10.689904       16.614230    2.670854      5731  ...  0.866691   \n",
      "18   2.216440        6.306638    4.298769      1398  ...  0.064471   \n",
      "19  10.679288       13.130683    3.645299      4494  ...  0.887299   \n",
      "20   2.161263        6.098785    3.500739      1518  ...  0.057142   \n",
      "21  10.530940       12.031145    3.823677      4362  ...  0.794245   \n",
      "22   8.005106       12.849689    2.319417      5816  ...  0.515026   \n",
      "23   3.615640        9.650187    1.364621      5409  ...  0.079851   \n",
      "24  10.400811       15.947530    2.550660      5746  ...  0.755091   \n",
      "25   2.110456        6.153296    6.296399      1108  ...  0.076812   \n",
      "26   6.188550        8.804952    2.988943      4347  ...  0.257187   \n",
      "27   1.583746        4.494518   -0.206551      1203  ...  0.014043   \n",
      "28   6.747845        9.429202    3.711212      4175  ...  0.366469   \n",
      "29   7.410645       12.052936    2.227687      5797  ...  0.415540   \n",
      "30   3.634937        9.668681    1.307521      5431  ...  0.077599   \n",
      "31  11.230070       17.104450    2.574422      5762  ...  0.880464   \n",
      "32   2.104846        6.030585    4.620818      1296  ...  0.064110   \n",
      "33   7.586834       10.202457    3.225487      4450  ...  0.405392   \n",
      "34   2.023921        5.716723    3.373368      1368  ...  0.052735   \n",
      "35  10.061686       11.744607    4.211453      4329  ...  0.988711   \n",
      "36   7.049944       11.583273    2.200406      5784  ...  0.370946   \n",
      "37   3.763586        9.962101    1.421443      5457  ...  0.086959   \n",
      "38   2.087587        6.032907    4.565189      1282  ...  0.063755   \n",
      "39   6.681397       12.428852    3.160567      4356  ...  0.313703   \n",
      "40   2.010437        5.728833    3.247667      1396  ...  0.051698   \n",
      "41   7.558464        9.395668    3.849089      4236  ...  0.492964   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max        sv_min  \\\n",
      "0       40.335403  1086.788940  success   6.351016  5.396172e-01   \n",
      "1       43.879704  1027.313843  success   6.624176  6.702299e-01   \n",
      "2      498.253479   109.545525  success  22.321592  5.324416e-01   \n",
      "3       38.612408  1140.284180  success   6.213888  5.606607e-01   \n",
      "4      595.779114    10.983719  success  24.408587  2.215775e-06   \n",
      "5       41.244293   367.517548  success   6.422172  1.690832e-04   \n",
      "6      620.159668    11.872713  success  24.903006  4.739448e-06   \n",
      "7       13.041498  1120.325439  success   3.611301  4.181476e-05   \n",
      "8       40.659092  1111.326782  success   6.376448  6.590129e-01   \n",
      "9      491.054871   111.105789  success  22.159758  5.108019e-01   \n",
      "10      37.942902  1165.711548  success   6.159781  5.346684e-01   \n",
      "11     712.817627     8.061022  success  26.698645  6.073339e-06   \n",
      "12      92.290550   153.282700  success   9.606797  1.287525e-05   \n",
      "13     673.573303     9.794077  success  25.953291  1.533200e-06   \n",
      "14      13.375615  1013.748840  success   3.657269  3.038347e-05   \n",
      "15      37.902214  1193.646729  success   6.156477  6.450626e-01   \n",
      "16     478.897400   113.835251  success  21.883724  5.071393e-01   \n",
      "17      35.825993  1238.748169  success   5.985482  5.413253e-01   \n",
      "18     700.472473     8.524056  success  26.466440  1.195924e-06   \n",
      "19      16.964720   872.837463  success   4.118825  1.670531e-04   \n",
      "20     663.531799    10.132889  success  25.759111  3.850392e-06   \n",
      "21      13.882154  1039.126831  success   3.725876  7.620265e-05   \n",
      "22      40.289024  1125.704590  success   6.347363  6.488391e-01   \n",
      "23     466.672424   116.575630  success  21.602602  5.102166e-01   \n",
      "24      34.142605  1306.427612  success   5.843167  5.390686e-01   \n",
      "25     823.423706     5.435855  success  28.695360  1.117687e-06   \n",
      "26      26.471663   499.890167  success   5.145062  4.877394e-06   \n",
      "27     688.499451     7.939143  success  26.239273  3.445655e-07   \n",
      "28      24.966919   505.446838  success   4.996691  6.921695e-05   \n",
      "29      42.309269  1073.477661  success   6.504558  6.382422e-01   \n",
      "30     457.015472   118.971489  success  21.377920  5.220850e-01   \n",
      "31      33.349846  1342.250122  success   5.774932  5.373365e-01   \n",
      "32     732.984741     7.037272  success  27.073690  7.359943e-07   \n",
      "33      22.118631   643.792542  success   4.703045  1.949904e-05   \n",
      "34     667.695129     9.129691  success  25.839798  4.137920e-06   \n",
      "35      14.698069   944.929443  success   3.833806  5.321610e-05   \n",
      "36      43.957249  1035.476807  success   6.630026  6.309476e-01   \n",
      "37     443.578461   122.335495  success  21.061302  4.983433e-01   \n",
      "38     776.060425     6.839413  success  27.857861  2.742670e-06   \n",
      "39      72.479881   186.515411  success   8.513512  1.463481e-04   \n",
      "40     707.205811     8.774707  success  26.593342  6.586047e-07   \n",
      "41      17.501116   745.443665  success   4.183434  1.903599e-04   \n",
      "\n",
      "          warning  weak_rank_loss        xmax       xmin  \n",
      "0   under-trained               0   40.335403  17.762970  \n",
      "1   under-trained               0   43.879704  14.157259  \n",
      "2                               0  498.253479  12.342011  \n",
      "3   under-trained               0   38.612408  17.241772  \n",
      "4                             163  595.779114   3.776859  \n",
      "5   under-trained               6   41.244293   7.087689  \n",
      "6                             150  620.159668   3.940760  \n",
      "7   under-trained               7   13.041498   8.612537  \n",
      "8   under-trained               0   40.659092  14.289001  \n",
      "9                               0  491.054871  12.173419  \n",
      "10  under-trained               0   37.942902  17.286358  \n",
      "11                            218  712.817627   3.110878  \n",
      "12  under-trained               5   92.290550   6.259271  \n",
      "13                            198  673.573303   3.234546  \n",
      "14  under-trained               6   13.375615   8.236437  \n",
      "15  under-trained               0   37.902214  14.797181  \n",
      "16                              0  478.897400  11.719819  \n",
      "17  under-trained               0   35.825993  17.051163  \n",
      "18                            201  700.472473   3.428758  \n",
      "19  under-trained               5   16.964720   7.420371  \n",
      "20                            187  663.531799   3.214602  \n",
      "21  under-trained               6   13.882154   7.541637  \n",
      "22  under-trained               0   40.289024  15.478435  \n",
      "23                              0  466.672424  12.228852  \n",
      "24  under-trained               0   34.142605  16.480700  \n",
      "25                            258  823.423706   3.656087  \n",
      "26  under-trained               7   26.471663   5.569308  \n",
      "27   over-trained             234  688.499451   0.131253  \n",
      "28  under-trained              11   24.966919   6.422902  \n",
      "29  under-trained               0   42.309269  14.991612  \n",
      "30                              0  457.015472  11.846412  \n",
      "31  under-trained               0   33.349846  16.670696  \n",
      "32                            235  732.984741   3.158190  \n",
      "33  under-trained               5   22.118631   6.405593  \n",
      "34                            219  667.695129   2.812478  \n",
      "35  under-trained               6   14.698069   7.918472  \n",
      "36  under-trained               0   43.957249  14.872040  \n",
      "37                              0  443.578461  12.573236  \n",
      "38                            237  776.060425   3.217538  \n",
      "39  under-trained               5   72.479881   5.971832  \n",
      "40                            212  707.205811   2.773740  \n",
      "41  under-trained               7   17.501116   6.858391  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.53.mlp.up_proj  0.042922  8192  22016  2.6875   \n",
      "1          5     model.layers.54.mlp.down_proj  0.020397  8192  22016  2.6875   \n",
      "2          6     model.layers.54.mlp.gate_proj  0.046873  8192  22016  2.6875   \n",
      "3          7       model.layers.54.mlp.up_proj  0.039512  8192  22016  2.6875   \n",
      "4          9  model.layers.54.self_attn.k_proj  0.028581  8192   8192  1.0000   \n",
      "5         10  model.layers.54.self_attn.o_proj  0.020515  8192   8192  1.0000   \n",
      "6         11  model.layers.54.self_attn.q_proj  0.033021  8192   8192  1.0000   \n",
      "7         12  model.layers.54.self_attn.v_proj  0.036207  8192   8192  1.0000   \n",
      "8         14     model.layers.55.mlp.down_proj  0.020540  8192  22016  2.6875   \n",
      "9         15     model.layers.55.mlp.gate_proj  0.046551  8192  22016  2.6875   \n",
      "10        16       model.layers.55.mlp.up_proj  0.044884  8192  22016  2.6875   \n",
      "11        18  model.layers.55.self_attn.k_proj  0.037415  8192   8192  1.0000   \n",
      "12        19  model.layers.55.self_attn.o_proj  0.014400  8192   8192  1.0000   \n",
      "13        20  model.layers.55.self_attn.q_proj  0.039755  8192   8192  1.0000   \n",
      "14        21  model.layers.55.self_attn.v_proj  0.043371  8192   8192  1.0000   \n",
      "15        23     model.layers.56.mlp.down_proj  0.021473  8192  22016  2.6875   \n",
      "16        24     model.layers.56.mlp.gate_proj  0.044453  8192  22016  2.6875   \n",
      "17        25       model.layers.56.mlp.up_proj  0.041474  8192  22016  2.6875   \n",
      "18        27  model.layers.56.self_attn.k_proj  0.034386  8192   8192  1.0000   \n",
      "19        28  model.layers.56.self_attn.o_proj  0.039002  8192   8192  1.0000   \n",
      "20        29  model.layers.56.self_attn.q_proj  0.038919  8192   8192  1.0000   \n",
      "21        30  model.layers.56.self_attn.v_proj  0.046705  8192   8192  1.0000   \n",
      "22        32     model.layers.57.mlp.down_proj  0.020775  8192  22016  2.6875   \n",
      "23        33     model.layers.57.mlp.gate_proj  0.044426  8192  22016  2.6875   \n",
      "24        34       model.layers.57.mlp.up_proj  0.039838  8192  22016  2.6875   \n",
      "25        36  model.layers.57.self_attn.k_proj  0.031650  8192   8192  1.0000   \n",
      "26        37  model.layers.57.self_attn.o_proj  0.037512  8192   8192  1.0000   \n",
      "27        38  model.layers.57.self_attn.q_proj  0.034504  8192   8192  1.0000   \n",
      "28        39  model.layers.57.self_attn.v_proj  0.022096  8192   8192  1.0000   \n",
      "29        41     model.layers.58.mlp.down_proj  0.018879  8192  22016  2.6875   \n",
      "30        42     model.layers.58.mlp.gate_proj  0.041462  8192  22016  2.6875   \n",
      "31        43       model.layers.58.mlp.up_proj  0.045015  8192  22016  2.6875   \n",
      "32        45  model.layers.58.self_attn.k_proj  0.037251  8192   8192  1.0000   \n",
      "33        46  model.layers.58.self_attn.o_proj  0.041495  8192   8192  1.0000   \n",
      "34        47  model.layers.58.self_attn.q_proj  0.038182  8192   8192  1.0000   \n",
      "35        48  model.layers.58.self_attn.v_proj  0.061405  8192   8192  1.0000   \n",
      "36        49     model.layers.59.mlp.down_proj  0.015864  8192  22016  2.6875   \n",
      "37        50     model.layers.59.mlp.gate_proj  0.036472  8192  22016  2.6875   \n",
      "38        51  model.layers.59.self_attn.k_proj  0.032570  8192   8192  1.0000   \n",
      "39        52  model.layers.59.self_attn.o_proj  0.018646  8192   8192  1.0000   \n",
      "40        53  model.layers.59.self_attn.q_proj  0.034669  8192   8192  1.0000   \n",
      "41        54  model.layers.59.self_attn.v_proj  0.022396  8192   8192  1.0000   \n",
      "\n",
      "        alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   12.190471       18.539977    2.623745      5774  ...  1.071853   \n",
      "1    6.698126       10.998022    2.165873      5762  ...  0.330639   \n",
      "2    3.838922       10.075022    1.439405      5479  ...  0.090136   \n",
      "3   12.006117       18.373087    2.560399      5785  ...  1.004716   \n",
      "4    2.269012        6.390620    4.143437      1604  ...  0.065357   \n",
      "5   11.675089       13.333332    3.777076      4504  ...  1.101051   \n",
      "6    2.219499        6.187919    3.435458      1707  ...  0.058403   \n",
      "7   15.839185       18.011054    4.065029      4420  ...  1.899963   \n",
      "8    6.327156       10.445668    2.113326      5744  ...  0.288906   \n",
      "9    6.152045       16.143948    3.357909      5502  ...  0.451864   \n",
      "10  12.152926       19.455423    2.524870      5801  ...  1.013902   \n",
      "11   2.068203        5.835445    3.585738      1382  ...  0.056615   \n",
      "12   6.318955        7.626807    3.233527      4428  ...  0.319010   \n",
      "13   2.085456        5.868340    3.531758      1523  ...  0.055830   \n",
      "14   6.679619        8.914023    3.150853      4356  ...  0.304460   \n",
      "15   6.078725       10.048795    2.162059      5720  ...  0.281285   \n",
      "16   6.161045       16.113011    3.226746      5520  ...  0.434638   \n",
      "17  12.363390       19.163950    2.491578      5812  ...  1.012331   \n",
      "18   2.062724        5.932467    3.971120      1294  ...  0.059689   \n",
      "19   6.237726       13.263424    3.118374      4332  ...  0.280772   \n",
      "20   2.050969        5.814116    3.383827      1478  ...  0.054056   \n",
      "21   7.063234        9.815345    3.325301      4276  ...  0.340545   \n",
      "22   5.695037        9.365601    2.075386      5701  ...  0.238971   \n",
      "23   6.391557       16.670462    3.206165      5538  ...  0.464031   \n",
      "24  12.264846       19.740984    2.432723      5821  ...  0.955472   \n",
      "25   2.136936        6.119230    4.242826      1365  ...  0.063163   \n",
      "26   6.994786       13.347264    3.248047      4356  ...  0.346109   \n",
      "27   2.099197        5.910373    3.485574      1519  ...  0.055660   \n",
      "28   8.155033        9.891457    3.800493      4299  ...  0.563896   \n",
      "29   5.401090        8.883188    2.068590      5676  ...  0.220330   \n",
      "30   6.414627       16.666275    3.121273      5560  ...  0.460924   \n",
      "31  11.930342       19.684221    2.368409      5837  ...  0.880792   \n",
      "32   1.548039        4.461168   -0.285759      1176  ...  0.011682   \n",
      "33   7.763538       10.217191    3.477426      4315  ...  0.433881   \n",
      "34   1.583407        4.513635   -0.272182      1334  ...  0.012079   \n",
      "35  11.502884       12.896544    3.694448      4297  ...  0.798519   \n",
      "36   5.152625        8.524026    1.999151      5655  ...  0.195757   \n",
      "37   6.253243       16.134088    2.907785      5579  ...  0.405296   \n",
      "38   2.292926        6.492254    4.901601      1610  ...  0.072504   \n",
      "39   6.550447       10.282008    3.688050      4327  ...  0.397476   \n",
      "40   2.231886        6.218931    3.883656      1763  ...  0.062299   \n",
      "41   6.596858        8.651717    3.340745      4304  ...  0.328659   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max        sv_min  \\\n",
      "0       33.178604  1355.328369  success   5.760087  5.288681e-01   \n",
      "1       43.848545  1040.947632  success   6.621823  6.356344e-01   \n",
      "2      421.153870   128.511871  success  20.522034  5.257248e-01   \n",
      "3       33.908649  1332.659790  success   5.823113  5.374695e-01   \n",
      "4      655.355530     9.994272  success  25.599913  3.895309e-06   \n",
      "5       13.868595  1094.260742  success   3.724056  4.797168e-05   \n",
      "6      613.733154    11.929603  success  24.773638  1.579406e-06   \n",
      "7       13.712606  1089.875854  success   3.703054  1.544770e-04   \n",
      "8       44.763744  1020.608337  success   6.690571  6.270093e-01   \n",
      "9      420.881470   128.488113  success  20.515396  5.208111e-01   \n",
      "10      39.891819  1136.145142  success   6.315997  5.338550e-01   \n",
      "11     662.985962     8.803961  success  25.748514  7.678087e-07   \n",
      "12      16.105444   926.607910  success   4.013159  8.829866e-05   \n",
      "13     651.531738    10.086526  success  25.525120  5.127511e-06   \n",
      "14      21.602833   678.351746  success   4.647885  9.082937e-05   \n",
      "15      44.989265  1019.261047  success   6.707404  6.149580e-01   \n",
      "16     412.386932   130.730103  success  20.307312  4.747960e-01   \n",
      "17      35.485939  1284.344116  success   5.957007  4.915588e-01   \n",
      "18     751.683594     7.161320  success  27.416849  3.915644e-07   \n",
      "19     133.759186   103.991127  success  11.565431  6.688868e-05   \n",
      "20     683.618408     9.292049  success  26.146097  2.521655e-06   \n",
      "21      24.526688   552.804871  success   4.952443  3.408193e-05   \n",
      "22      44.108238  1039.443237  success   6.641403  6.027654e-01   \n",
      "23     405.695953   132.899368  success  20.141895  4.763132e-01   \n",
      "24      40.696613  1122.261353  success   6.379390  4.886996e-01   \n",
      "25     730.386597     7.630275  success  27.025665  3.865529e-06   \n",
      "26      80.941887   173.714920  success   8.996771  1.026693e-04   \n",
      "27     653.942810     9.895990  success  25.572306  6.769558e-07   \n",
      "28      16.327761   838.542969  success   4.040762  7.038598e-05   \n",
      "29      44.126865  1040.404175  success   6.642806  6.083749e-01   \n",
      "30     396.430969   135.904007  success  19.910574  4.750054e-01   \n",
      "31      44.661087  1025.931152  success   6.682895  5.022544e-01   \n",
      "32     761.760132     7.153835  success  27.600002  3.103783e-06   \n",
      "33      20.703712   645.043579  success   4.550133  7.590159e-05   \n",
      "34     708.897339     8.756432  success  26.625126  2.833929e-07   \n",
      "35      13.217751   997.321411  success   3.635623  1.932138e-05   \n",
      "36      45.113613  1019.848755  success   6.716667  6.158004e-01   \n",
      "37     380.290405   141.265579  success  19.501036  4.851779e-01   \n",
      "38     678.308655     9.438763  success  26.044359  3.608304e-06   \n",
      "39      37.124893   378.318848  success   6.093020  4.096341e-05   \n",
      "40     611.506836    11.753681  success  24.728664  3.072224e-06   \n",
      "41      20.487577   674.120422  success   4.526320  6.549206e-05   \n",
      "\n",
      "          warning  weak_rank_loss        xmax       xmin  \n",
      "0   under-trained               0   33.178604  17.017946  \n",
      "1   under-trained               0   43.848545  14.722772  \n",
      "2                               0  421.153870  12.659799  \n",
      "3   under-trained               0   33.908649  16.751621  \n",
      "4                             170  655.355530   3.646614  \n",
      "5   under-trained               5   13.868595   7.849355  \n",
      "6                             164  613.733154   3.449884  \n",
      "7   under-trained               5   13.712606   8.255425  \n",
      "8   under-trained               0   44.763744  14.441962  \n",
      "9   under-trained               0  420.881470  25.312651  \n",
      "10  under-trained               0   39.891819  16.604866  \n",
      "11                            183  662.985962   2.853573  \n",
      "12  under-trained               4   16.105444   6.726720  \n",
      "13                            165  651.531738   3.174977  \n",
      "14  under-trained               5   21.602833   6.457102  \n",
      "15  under-trained               0   44.989265  14.769215  \n",
      "16  under-trained               0  412.386932  24.371735  \n",
      "17  under-trained               0   35.485939  16.511078  \n",
      "18                            206  751.683594   2.877393  \n",
      "19  under-trained               7  133.759186   6.073450  \n",
      "20                            186  683.618408   2.951882  \n",
      "21  under-trained               8   24.526688   6.262225  \n",
      "22                              0   44.108238  14.280907  \n",
      "23  under-trained               0  405.695953  24.240046  \n",
      "24  under-trained               0   40.696613  16.219835  \n",
      "25                            211  730.386597   3.163652  \n",
      "26  under-trained               5   80.941887   6.362700  \n",
      "27                            193  653.942810   3.085623  \n",
      "28  under-trained               6   16.327761   7.117374  \n",
      "29                              0   44.126865  14.263942  \n",
      "30  under-trained               0  396.430969  23.661997  \n",
      "31  under-trained               0   44.661087  15.910554  \n",
      "32   over-trained             233  761.760132   0.074570  \n",
      "33  under-trained               8   20.703712   6.415160  \n",
      "34   over-trained             204  708.897339   0.107166  \n",
      "35  under-trained              10   13.217751   6.680495  \n",
      "36                              0   45.113613  13.905453  \n",
      "37  under-trained               0  380.290405  22.195570  \n",
      "38                            196  678.308655   4.158534  \n",
      "39  under-trained               5   37.124893   7.108598  \n",
      "40                            179  611.506836   3.783958  \n",
      "41  under-trained               6   20.487577   6.404911  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N       Q  \\\n",
      "0          2       model.layers.71.mlp.up_proj  0.046190  8192  22016  2.6875   \n",
      "1          5     model.layers.72.mlp.down_proj  0.013626  8192  22016  2.6875   \n",
      "2          6     model.layers.72.mlp.gate_proj  0.034239  8192  22016  2.6875   \n",
      "3          7       model.layers.72.mlp.up_proj  0.047958  8192  22016  2.6875   \n",
      "4          9  model.layers.72.self_attn.k_proj  0.018071  8192   8192  1.0000   \n",
      "5         10  model.layers.72.self_attn.o_proj  0.018320  8192   8192  1.0000   \n",
      "6         11  model.layers.72.self_attn.q_proj  0.018569  8192   8192  1.0000   \n",
      "7         12  model.layers.72.self_attn.v_proj  0.015398  8192   8192  1.0000   \n",
      "8         14     model.layers.73.mlp.down_proj  0.014164  8192  22016  2.6875   \n",
      "9         15     model.layers.73.mlp.gate_proj  0.033502  8192  22016  2.6875   \n",
      "10        16       model.layers.73.mlp.up_proj  0.050643  8192  22016  2.6875   \n",
      "11        18  model.layers.73.self_attn.k_proj  0.013593  8192   8192  1.0000   \n",
      "12        19  model.layers.73.self_attn.o_proj  0.020599  8192   8192  1.0000   \n",
      "13        20  model.layers.73.self_attn.q_proj  0.015065  8192   8192  1.0000   \n",
      "14        21  model.layers.73.self_attn.v_proj  0.024594  8192   8192  1.0000   \n",
      "15        23     model.layers.74.mlp.down_proj  0.011472  8192  22016  2.6875   \n",
      "16        24     model.layers.74.mlp.gate_proj  0.030950  8192  22016  2.6875   \n",
      "17        25       model.layers.74.mlp.up_proj  0.031092  8192  22016  2.6875   \n",
      "18        27  model.layers.74.self_attn.k_proj  0.015382  8192   8192  1.0000   \n",
      "19        28  model.layers.74.self_attn.o_proj  0.019393  8192   8192  1.0000   \n",
      "20        29  model.layers.74.self_attn.q_proj  0.011122  8192   8192  1.0000   \n",
      "21        30  model.layers.74.self_attn.v_proj  0.008036  8192   8192  1.0000   \n",
      "22        32     model.layers.75.mlp.down_proj  0.013749  8192  22016  2.6875   \n",
      "23        33     model.layers.75.mlp.gate_proj  0.031357  8192  22016  2.6875   \n",
      "24        34       model.layers.75.mlp.up_proj  0.044724  8192  22016  2.6875   \n",
      "25        36  model.layers.75.self_attn.k_proj  0.015889  8192   8192  1.0000   \n",
      "26        37  model.layers.75.self_attn.o_proj  0.042664  8192   8192  1.0000   \n",
      "27        38  model.layers.75.self_attn.q_proj  0.013946  8192   8192  1.0000   \n",
      "28        39  model.layers.75.self_attn.v_proj  0.016316  8192   8192  1.0000   \n",
      "29        41     model.layers.76.mlp.down_proj  0.015891  8192  22016  2.6875   \n",
      "30        42     model.layers.76.mlp.gate_proj  0.031400  8192  22016  2.6875   \n",
      "31        43       model.layers.76.mlp.up_proj  0.024983  8192  22016  2.6875   \n",
      "32        45  model.layers.76.self_attn.k_proj  0.015753  8192   8192  1.0000   \n",
      "33        46  model.layers.76.self_attn.o_proj  0.017290  8192   8192  1.0000   \n",
      "34        47  model.layers.76.self_attn.q_proj  0.018552  8192   8192  1.0000   \n",
      "35        48  model.layers.76.self_attn.v_proj  0.018435  8192   8192  1.0000   \n",
      "36        49     model.layers.77.mlp.down_proj  0.020076  8192  22016  2.6875   \n",
      "37        50     model.layers.77.mlp.gate_proj  0.025974  8192  22016  2.6875   \n",
      "38        51  model.layers.77.self_attn.k_proj  0.017060  8192   8192  1.0000   \n",
      "39        52  model.layers.77.self_attn.o_proj  0.037486  8192   8192  1.0000   \n",
      "40        53  model.layers.77.self_attn.q_proj  0.014582  8192   8192  1.0000   \n",
      "41        54  model.layers.77.self_attn.v_proj  0.029859  8192   8192  1.0000   \n",
      "\n",
      "       alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   4.936475       10.154150    2.726198      5927  ...  0.707012   \n",
      "1   4.882525        8.469041    2.317512      5360  ...  0.200226   \n",
      "2   5.806111       14.079391    1.757497      5779  ...  0.199735   \n",
      "3   7.361889       15.757969    1.717863      5920  ...  0.267884   \n",
      "4   2.585235        6.836145    3.650821      2535  ...  0.074895   \n",
      "5   3.885372        8.268485    3.992652      3919  ...  0.175924   \n",
      "6   2.924216        7.643148   14.958065      2657  ...  0.264311   \n",
      "7   4.064671        7.249443    3.598237      4062  ...  0.171053   \n",
      "8   4.799930        8.491565    2.310581      5314  ...  0.191925   \n",
      "9   5.524593       13.636422    1.649003      5778  ...  0.173002   \n",
      "10  6.837195       15.931088    1.646021      5909  ...  0.230376   \n",
      "11  2.573361        6.891950    3.550069      2603  ...  0.074251   \n",
      "12  4.143507        9.361649    3.292934      3821  ...  0.148351   \n",
      "13  2.619354        6.930444    3.691491      2725  ...  0.079395   \n",
      "14  4.091938        9.257362    3.677315      3904  ...  0.170724   \n",
      "15  4.746666        8.461284    2.343256      5252  ...  0.188515   \n",
      "16  5.418203       13.095244    1.627776      5781  ...  0.166046   \n",
      "17  4.125701        8.856301    3.092477      5910  ...  0.570672   \n",
      "18  2.737007        7.083588    3.557842      2777  ...  0.080813   \n",
      "19  3.869440        7.543156    3.340894      3921  ...  0.142408   \n",
      "20  2.738848        7.050495    4.494397      2855  ...  0.098601   \n",
      "21  4.406208        8.367491    3.786620      4078  ...  0.207680   \n",
      "22  5.108817        9.151799    2.620415      5212  ...  0.240040   \n",
      "23  3.778780        9.278661    3.125960      5760  ...  0.344665   \n",
      "24  5.868388       13.303449    1.525340      5881  ...  0.171375   \n",
      "25  2.809377        7.271663    4.234059      2874  ...  0.094968   \n",
      "26  3.600082        6.603758    3.716953      3757  ...  0.131829   \n",
      "27  2.921984        7.497446    8.346253      2881  ...  0.169221   \n",
      "28  4.186321        7.267209    3.645082      3998  ...  0.174609   \n",
      "29  4.799005        8.965058    2.414481      5136  ...  0.191879   \n",
      "30  4.629549       11.490669    1.396302      5726  ...  0.115122   \n",
      "31  3.532451        8.177865    3.643719      5841  ...  0.434312   \n",
      "32  2.636606        6.819994    3.387968      2821  ...  0.074935   \n",
      "33  3.956371        8.251129    3.581321      3807  ...  0.150867   \n",
      "34  2.812787        7.104951    8.576986      2847  ...  0.160859   \n",
      "35  4.175074        9.126212    3.169993      4015  ...  0.152408   \n",
      "36  4.572526        8.914070    2.240364      5052  ...  0.160089   \n",
      "37  3.280485        8.333076    3.243611      5653  ...  0.243101   \n",
      "38  2.497301        6.597639    3.268619      2526  ...  0.067163   \n",
      "39  3.594676        8.261241    6.268347      3747  ...  0.312362   \n",
      "40  2.755675        7.137335    9.576626      2579  ...  0.163010   \n",
      "41  4.730481        8.442644    3.447271      4041  ...  0.193157   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max        sv_min  \\\n",
      "0      114.015480   419.726929  success  10.677803  5.062721e-01   \n",
      "1       54.270252   867.953613  success   7.366835  6.606880e-01   \n",
      "2      266.027344   194.836121  success  16.310345  4.752336e-01   \n",
      "3      138.190750   346.662231  success  11.755456  5.018540e-01   \n",
      "4      440.862793    23.106606  success  20.996733  3.832700e-06   \n",
      "5      134.309464   128.655487  success  11.589196  3.070508e-05   \n",
      "6      410.906097    25.940250  success  20.270819  8.280772e-06   \n",
      "7       60.747032   281.341553  success   7.794038  1.937023e-05   \n",
      "8       58.762672   801.156433  success   7.665681  6.576930e-01   \n",
      "9      293.976349   176.435349  success  17.145739  4.553584e-01   \n",
      "10     213.826645   224.733597  success  14.622812  4.782730e-01   \n",
      "11     476.640076    22.057055  success  21.832088  7.375265e-06   \n",
      "12     181.699539    81.413483  success  13.479597  1.795639e-05   \n",
      "13     442.445770    24.590355  success  21.034395  2.686124e-06   \n",
      "14     182.953888    80.913055  success  13.526045  1.616964e-05   \n",
      "15      60.614143   780.613708  success   7.785509  6.660030e-01   \n",
      "16     261.154785   197.670670  success  16.160284  4.329045e-01   \n",
      "17     140.157654   343.635132  success  11.838820  4.423386e-01   \n",
      "18     387.327332    26.988258  success  19.680634  8.181858e-06   \n",
      "19      89.005753   179.877197  success   9.434286  1.190302e-05   \n",
      "20     375.193451    29.144903  success  19.369911  4.077133e-07   \n",
      "21      79.254341   203.053162  success   8.902491  7.080709e-05   \n",
      "22      61.854778   762.068237  success   7.864781  6.509652e-01   \n",
      "23     285.407196   181.512238  success  16.893999  3.954839e-01   \n",
      "24     184.913315   260.911438  success  13.598284  3.970681e-01   \n",
      "25     387.573853    30.346512  success  19.686895  7.157418e-06   \n",
      "26      68.286621   252.999603  success   8.263572  5.269212e-05   \n",
      "27     368.023071    32.860615  success  19.183928  1.081123e-06   \n",
      "28      54.442966   317.597900  success   7.378548  5.205389e-06   \n",
      "29      73.808685   638.155212  success   8.591198  6.051219e-01   \n",
      "30     303.408752   171.398041  success  17.418633  3.717777e-01   \n",
      "31     206.570572   233.906586  success  14.372563  3.702208e-01   \n",
      "32     386.061920    30.214294  success  19.648458  2.749423e-06   \n",
      "33     121.767075   128.714325  success  11.034812  2.442973e-05   \n",
      "34     335.696899    35.549099  success  18.322033  1.403979e-06   \n",
      "35     153.419434   102.636093  success  12.386260  1.181786e-04   \n",
      "36      89.019409   527.845398  success   9.435010  5.655231e-01   \n",
      "37     346.893311   151.104675  success  18.625072  3.772298e-01   \n",
      "38     438.437927    23.404993  success  20.938910  5.214837e-06   \n",
      "39     198.695465    76.563171  success  14.095938  1.208997e-05   \n",
      "40     389.089630    26.787823  success  19.725355  3.915537e-06   \n",
      "41      60.916153   251.572296  success   7.804880  6.108703e-05   \n",
      "\n",
      "          warning  weak_rank_loss        xmax       xmin  \n",
      "0                               0  114.015480  18.707150  \n",
      "1                               0   54.270252  16.060902  \n",
      "2                               0  266.027344  14.134963  \n",
      "3   under-trained               0  138.190750  12.830813  \n",
      "4                              95  440.862793   5.091532  \n",
      "5                               9  134.309464   9.387218  \n",
      "6                              77  410.906097  20.046207  \n",
      "7                               7   60.747032   8.462320  \n",
      "8                               0   58.762672  16.010118  \n",
      "9                               0  293.976349  13.456323  \n",
      "10  under-trained               0  213.826645  12.449963  \n",
      "11                            102  476.640076   5.126222  \n",
      "12                             14  181.699539   6.765815  \n",
      "13                             86  442.445770   5.498182  \n",
      "14                             14  182.953888   7.467336  \n",
      "15                              0   60.614143  16.278749  \n",
      "16                              0  261.154785  13.260344  \n",
      "17                              0  140.157654  20.980301  \n",
      "18                             80  387.327332   5.108899  \n",
      "19                              9   89.005753   7.422143  \n",
      "20                             68  375.193451   6.599575  \n",
      "21                              8   79.254341   8.336405  \n",
      "22                              0   61.854778  17.807674  \n",
      "23                              0  285.407196  22.783670  \n",
      "24                              0  184.913315  11.787933  \n",
      "25                             59  387.573853   6.724484  \n",
      "26                             11   68.286621   8.802849  \n",
      "27                             51  368.023071  12.988870  \n",
      "28                              9   54.442966   8.658817  \n",
      "29                              0   73.808685  16.607893  \n",
      "30                              0  303.408752  11.890024  \n",
      "31                              0  206.570572  24.301174  \n",
      "32                             92  386.061920   5.462202  \n",
      "33                              9  121.767075   7.723948  \n",
      "34                             82  335.696899  13.151891  \n",
      "35                              9  153.419434   6.971052  \n",
      "36                              0   89.019409  15.565357  \n",
      "37                              0  346.893311  23.803549  \n",
      "38                            123  438.437927   4.643648  \n",
      "39                             10  198.695465  12.487192  \n",
      "40                            109  389.089630  12.748432  \n",
      "41                              8   60.916153   7.303958  \n",
      "\n",
      "[42 rows x 36 columns]\n",
      "    layer_id                              name         D     M      N  \\\n",
      "0          1                           lm_head  0.027352  8192  32000   \n",
      "1          3       model.layers.77.mlp.up_proj  0.027209  8192  22016   \n",
      "2          6     model.layers.78.mlp.down_proj  0.025920  8192  22016   \n",
      "3          7     model.layers.78.mlp.gate_proj  0.015345  8192  22016   \n",
      "4          8       model.layers.78.mlp.up_proj  0.023269  8192  22016   \n",
      "5         10  model.layers.78.self_attn.k_proj  0.014704  8192   8192   \n",
      "6         11  model.layers.78.self_attn.o_proj  0.027398  8192   8192   \n",
      "7         12  model.layers.78.self_attn.q_proj  0.014064  8192   8192   \n",
      "8         13  model.layers.78.self_attn.v_proj  0.012177  8192   8192   \n",
      "9         15     model.layers.79.mlp.down_proj  0.036143  8192  22016   \n",
      "10        16     model.layers.79.mlp.gate_proj  0.016221  8192  22016   \n",
      "11        17       model.layers.79.mlp.up_proj  0.019784  8192  22016   \n",
      "12        19  model.layers.79.self_attn.k_proj  0.019547  8192   8192   \n",
      "13        20  model.layers.79.self_attn.o_proj  0.019055  8192   8192   \n",
      "14        21  model.layers.79.self_attn.q_proj  0.022925  8192   8192   \n",
      "15        22  model.layers.79.self_attn.v_proj  0.017327  8192   8192   \n",
      "\n",
      "          Q     alpha  alpha_weighted  detX_delta  detX_num  ...     sigma  \\\n",
      "0   3.90625  5.061738       17.923031    1.394195      5911  ...  0.145902   \n",
      "1   2.68750  3.400289        8.224455    3.166239      5760  ...  0.291078   \n",
      "2   2.68750  3.813598        8.245813    1.706517      4910  ...  0.094900   \n",
      "3   2.68750  3.052951        8.123353    3.265439      5490  ...  0.188989   \n",
      "4   2.68750  3.173622        8.302938    3.032549      5580  ...  0.198424   \n",
      "5   1.00000  2.699918        6.932104    4.473640      2613  ...  0.089969   \n",
      "6   1.00000  4.346972        8.256604    4.763735      3664  ...  0.217869   \n",
      "7   1.00000  2.688382        6.783692    6.974201      2640  ...  0.124809   \n",
      "8   1.00000  4.858896        7.955616    4.524739      3922  ...  0.270177   \n",
      "9   2.68750  3.496712        9.232484    1.598924      4589  ...  0.078561   \n",
      "10  2.68750  2.613846        7.800085    4.445918      5207  ...  0.192892   \n",
      "11  2.68750  2.607327        7.769265    3.898892      5237  ...  0.168494   \n",
      "12  1.00000  2.390144        6.853723    4.629880      1714  ...  0.073471   \n",
      "13  1.00000  2.551687        5.611187    6.462101      2841  ...  0.127548   \n",
      "14  1.00000  2.319152        6.309348    4.710529      1795  ...  0.071228   \n",
      "15  1.00000  3.399362        6.801823    5.691766      3098  ...  0.168402   \n",
      "\n",
      "    spectral_norm  stable_rank   status     sv_max    sv_min  warning  \\\n",
      "0     3474.438721    21.024902  success  58.944370  0.436861            \n",
      "1      262.271851   184.771835  success  16.194809  0.350356            \n",
      "2      145.282471   321.829407  success  12.053318  0.531362            \n",
      "3      457.951904   116.387314  success  21.399811  0.392717            \n",
      "4      413.269745   118.233337  success  20.329037  0.384651            \n",
      "5      369.422729    27.921593  success  19.220373  0.000002            \n",
      "6       79.321747   180.779953  success   8.906276  0.000050            \n",
      "7      333.685028    31.319235  success  18.267048  0.000006            \n",
      "8       43.384045   333.371674  success   6.586657  0.000009            \n",
      "9      436.850952   108.218369  success  20.900980  0.484391            \n",
      "10     964.141052    57.986488  success  31.050621  0.411479            \n",
      "11     954.511292    53.880451  success  30.895166  0.380434            \n",
      "12     737.044922    10.131502  success  27.148571  0.000004            \n",
      "13     158.128708    54.522148  success  12.574924  0.000021            \n",
      "14     525.461731    14.929740  success  22.922953  0.000001            \n",
      "15     100.210205    82.129959  success  10.010505  0.000019            \n",
      "\n",
      "    weak_rank_loss         xmax       xmin  \n",
      "0                0  3474.438721  16.701729  \n",
      "1                0   262.271851  21.548733  \n",
      "2                0   145.282471  12.436589  \n",
      "3                0   457.951904  24.350775  \n",
      "4                0   413.269745  20.931797  \n",
      "5              126   369.422729   6.188843  \n",
      "6               11    79.321747   9.135341  \n",
      "7              114   333.685028   9.464305  \n",
      "8               11    43.384045   8.795117  \n",
      "9                0   436.850952  11.949289  \n",
      "10               0   964.141052  33.594059  \n",
      "11               0   954.511292  27.467093  \n",
      "12             230   737.044922   4.596298  \n",
      "13              29   158.128708   7.263025  \n",
      "14             200   525.461731   4.914339  \n",
      "15              23   100.210205   6.161250  \n",
      "\n",
      "[16 rows x 36 columns]\n"
     ]
    }
   ],
   "source": [
    "from safetensors import safe_open\n",
    "import pandas as pd\n",
    "\n",
    "df = None\n",
    "\n",
    "for i in range(1, 15):\n",
    "  filename = f\"./llama-65b_safetensors/model-{i:05}-of-00014.safetensors\"\n",
    "  state_dict = {}\n",
    "  with safe_open(filename, framework=\"pt\", device='cpu') as f:\n",
    "    for k in f.keys():\n",
    "      state_dict[k] = f.get_tensor(k)\n",
    "      # print(k)\n",
    "  \n",
    "  \n",
    "  watcher = WeightWatcher(model={\"state_dict\": state_dict})\n",
    "  details = watcher.analyze(detX = True)\n",
    "  \n",
    "  if df is None: df = details\n",
    "  else:          df = pd.concat([df, details])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14ee3f34-1a69-49cb-95a0-fab2478da888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_pickle(\"llama-65b-details\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5eb2ea1-4401-48ed-bcc6-a675ea5f7850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"llama-65b-details\", \"rb\") as fp:\n",
    "  df65 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9668645f-1073-4c65-bf74-a1db63ddd12d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df65.plot.scatter(x='alpha', y='detX_delta')\n",
    "plt.title(\"Test of Trace Log Norm condition \\n\"+r\"LLAMA 65b: $\\Delta\\lambda_{min}$ (PL fit) - (|det X|=1)\", fontdict={'fontsize': plot_utils.MEDIUM_SIZE})\n",
    "plt.ylim((-2, 25))\n",
    "plt.xlabel(r\"PL $\\alpha$\")\n",
    "plt.ylabel(r\"$\\Delta\\lambda_{min}$\")\n",
    "plt.axvline(2, linewidth=0.5, color=\"red\", zorder=-1)\n",
    "plt.axhline(0, linewidth=0.5, color=\"red\", zorder=-1)\n",
    "plt.savefig(\"LLAMA_65b_ESD_trends.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea02908-462b-4837-81c3-ab01191c1a73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
